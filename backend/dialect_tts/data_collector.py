#!/usr/bin/env python3
"""
ä¸Šæµ·èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
YouTubeã€ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã€å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä¸Šæµ·èªã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""

import os
import yt_dlp
import requests
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass
import whisper
from pydub import AudioSegment
import librosa
import soundfile as sf
import numpy as np

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AudioData:
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    file_path: str
    duration: float
    sample_rate: int
    text: str
    speaker_id: str
    quality_score: float
    source: str
    dialect_region: str

class ShanghaiDialectCollector:
    """ä¸Šæµ·èªæ–¹è¨€ãƒ‡ãƒ¼ã‚¿åé›†å™¨"""
    
    def __init__(self, output_dir: str = "shanghai_audio_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆWhisperï¼‰
        self.whisper_model = whisper.load_model("base")
        
        # åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†
        self.collected_data = []
        self.load_existing_data()
    
    def collect_youtube_data(self, search_queries: List[str], max_duration_hours: float = 10.0) -> List[AudioData]:
        """YouTubeã‹ã‚‰ä¸Šæµ·èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸ¬ YouTubeã‹ã‚‰ä¸Šæµ·èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")
        
        collected_data = []
        total_duration = 0.0
        
        for query in search_queries:
            if total_duration >= max_duration_hours * 3600:
                break
                
            logger.info(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
            
            # YouTubeæ¤œç´¢è¨­å®š
            ydl_opts = {
                'format': 'bestaudio/best',
                'outtmpl': str(self.output_dir / 'youtube' / '%(title)s.%(ext)s'),
                'extractaudio': True,
                'audioformat': 'wav',
                'audioquality': '0',  # æœ€é«˜å“è³ª
                'noplaylist': True,
                'max_downloads': 20,  # ã‚¯ã‚¨ãƒªã‚ãŸã‚Šæœ€å¤§20å‹•ç”»
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['zh', 'zh-CN', 'zh-TW'],
            }
            
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    # æ¤œç´¢çµæœã‚’å–å¾—
                    search_results = ydl.extract_info(
                        f"ytsearch20:{query}",
                        download=False
                    )
                    
                    if not search_results or 'entries' not in search_results:
                        continue
                    
                    for entry in search_results['entries']:
                        if total_duration >= max_duration_hours * 3600:
                            break
                            
                        if not entry:
                            continue
                            
                        try:
                            # å‹•ç”»æƒ…å ±ã‚’å–å¾—
                            video_info = ydl.extract_info(entry['url'], download=False)
                            
                            # ä¸Šæµ·èªã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                            if self.is_likely_shanghai_content(video_info):
                                # éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                                audio_data = self.download_and_process_video(entry['url'], video_info)
                                if audio_data:
                                    collected_data.append(audio_data)
                                    total_duration += audio_data.duration
                                    logger.info(f"âœ… åé›†å®Œäº†: {audio_data.file_path} ({audio_data.duration:.1f}s)")
                                    
                        except Exception as e:
                            logger.warning(f"âš ï¸ å‹•ç”»å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                            
            except Exception as e:
                logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
                continue
        
        logger.info(f"ğŸ‰ YouTubeåé›†å®Œäº†: {len(collected_data)}ä»¶, åˆè¨ˆ{total_duration/3600:.1f}æ™‚é–“")
        return collected_data
    
    def is_likely_shanghai_content(self, video_info: Dict) -> bool:
        """å‹•ç”»ãŒä¸Šæµ·èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¯èƒ½æ€§ã‚’åˆ¤å®š"""
        title = video_info.get('title', '').lower()
        description = video_info.get('description', '').lower()
        
        # ä¸Šæµ·èªé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        shanghai_keywords = [
            'ä¸Šæµ·è¯', 'ä¸Šæµ·èª', 'shanghai dialect', 'shanghainese',
            'æ²ªè¯­', 'æ²ªä¸Š', 'ä¸Šæµ·äºº', 'ä¸Šæµ·è¯æ•™å­¦',
            'ä¸Šæµ·æ–¹è¨€', 'ä¸Šæµ·è¯æ•™ç¨‹', 'ä¸Šæµ·è¯å­¦ä¹ '
        ]
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯èª¬æ˜ã«ä¸Šæµ·èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_to_check = f"{title} {description}"
        return any(keyword in text_to_check for keyword in shanghai_keywords)
    
    def download_and_process_video(self, url: str, video_info: Dict) -> Optional[AudioData]:
        """å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å
            temp_file = self.output_dir / 'temp' / f"temp_{int(time.time())}.wav"
            temp_file.parent.mkdir(exist_ok=True)
            
            # éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®š
            ydl_opts = {
                'format': 'bestaudio/best',
                'outtmpl': str(temp_file),
                'extractaudio': True,
                'audioformat': 'wav',
                'audioquality': '0',
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([url])
            
            if not temp_file.exists():
                return None
            
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            audio_data = self.process_audio_file(
                str(temp_file),
                video_info.get('title', 'Unknown'),
                'youtube'
            )
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            temp_file.unlink()
            
            return audio_data
            
        except Exception as e:
            logger.error(f"âŒ å‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def process_audio_file(self, file_path: str, title: str, source: str) -> Optional[AudioData]:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            audio, sample_rate = librosa.load(file_path, sr=None)
            duration = len(audio) / sample_rate
            
            # éŸ³å£°å“è³ªã‚’è©•ä¾¡
            quality_score = self.evaluate_audio_quality(audio, sample_rate)
            
            # å“è³ªãŒä½ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if quality_score < 0.3:
                logger.warning(f"âš ï¸ éŸ³å£°å“è³ªãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {file_path}")
                return None
            
            # Whisperã§éŸ³å£°èªè­˜
            result = self.whisper_model.transcribe(file_path, language='zh')
            text = result['text'].strip()
            
            # ä¸Šæµ·èªã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            if not self.is_likely_shanghai_text(text):
                logger.warning(f"âš ï¸ ä¸Šæµ·èªã§ã¯ãªã„å¯èƒ½æ€§: {file_path}")
                return None
            
            # æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            final_file = self.output_dir / 'processed' / f"shanghai_{int(time.time())}.wav"
            final_file.parent.mkdir(exist_ok=True)
            
            # éŸ³å£°ã‚’æ­£è¦åŒ–ã—ã¦ä¿å­˜
            normalized_audio = librosa.util.normalize(audio)
            sf.write(str(final_file), normalized_audio, sample_rate)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            audio_data = AudioData(
                file_path=str(final_file),
                duration=duration,
                sample_rate=sample_rate,
                text=text,
                speaker_id=f"speaker_{hash(title) % 10000}",
                quality_score=quality_score,
                source=source,
                dialect_region="shanghai"
            )
            
            return audio_data
            
        except Exception as e:
            logger.error(f"âŒ éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def evaluate_audio_quality(self, audio: np.ndarray, sample_rate: int) -> float:
        """éŸ³å£°å“è³ªã‚’è©•ä¾¡ï¼ˆ0-1ã®ã‚¹ã‚³ã‚¢ï¼‰"""
        try:
            # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡
            noise_level = np.std(audio)
            
            # ä¿¡å·å¯¾é›‘éŸ³æ¯”ã‚’è¨ˆç®—
            signal_power = np.mean(audio ** 2)
            noise_power = np.var(audio)
            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            
            # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆSNRãƒ™ãƒ¼ã‚¹ï¼‰
            quality_score = min(1.0, max(0.0, (snr + 10) / 40))  # -10dB to 30dB
            
            return quality_score
            
        except Exception as e:
            logger.warning(f"âš ï¸ å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def is_likely_shanghai_text(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒä¸Šæµ·èªã®å¯èƒ½æ€§ã‚’åˆ¤å®š"""
        # ä¸Šæµ·èªã®ç‰¹å¾´çš„ãªèªå½™
        shanghai_indicators = [
            'ä¾¬', 'é˜¿æ‹‰', 'ä¼Šæ‹‰', 'ä»Šæœ', 'æ˜æœ', 'æ˜¨æ—¥å­',
            'è›®å¥½', 'å†ä¼š', 'å•¥', 'å“ªèƒ½', 'å•¥åœ°æ–¹', 'è°¢è°¢ä¾¬',
            'åšç”Ÿæ´»', 'å›°è§‰', 'å±‹é‡Œ', 'å­¦å ‚', 'æ¿æ­', 'ä¼Šæ­'
        ]
        
        # ä¸Šæµ·èªã®èªå½™ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_lower = text.lower()
        shanghai_count = sum(1 for indicator in shanghai_indicators if indicator in text_lower)
        
        # ä¸Šæµ·èªã®èªå½™ãŒ2ã¤ä»¥ä¸Šå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæµ·èªã¨åˆ¤å®š
        return shanghai_count >= 2
    
    def collect_academic_data(self) -> List[AudioData]:
        """å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä¸Šæµ·èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸ“š å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä¸Šæµ·èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")
        
        # å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLï¼ˆä¾‹ï¼‰
        academic_sources = [
            "https://example-academic-db.com/shanghai-dialect",
            # å®Ÿéš›ã®å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’è¿½åŠ 
        ]
        
        collected_data = []
        
        for source_url in academic_sources:
            try:
                # å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                response = requests.get(source_url, timeout=30)
                if response.status_code == 200:
                    # ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€APIã®ä»•æ§˜ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
                    logger.info(f"âœ… å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—: {source_url}")
                    # ã“ã“ã§å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè£…
                    
            except Exception as e:
                logger.warning(f"âš ï¸ å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return collected_data
    
    def save_collected_data(self, audio_data_list: List[AudioData]):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        metadata_file = self.output_dir / "collected_metadata.json"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä¿å­˜
        metadata = []
        for data in audio_data_list:
            metadata.append({
                'file_path': data.file_path,
                'duration': data.duration,
                'sample_rate': data.sample_rate,
                'text': data.text,
                'speaker_id': data.speaker_id,
                'quality_score': data.quality_score,
                'source': data.source,
                'dialect_region': data.dialect_region
            })
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {metadata_file}")
    
    def load_existing_data(self):
        """æ—¢å­˜ã®åé›†ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        metadata_file = self.output_dir / "collected_metadata.json"
        
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                for data_dict in metadata:
                    self.collected_data.append(AudioData(**data_dict))
                
                logger.info(f"ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {len(self.collected_data)}ä»¶")
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ ä¸Šæµ·èªæ–¹è¨€ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")
    
    # åé›†å™¨ã‚’åˆæœŸåŒ–
    collector = ShanghaiDialectCollector()
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®šç¾©
    search_queries = [
        "ä¸Šæµ·è¯æ•™å­¦",
        "ä¸Šæµ·è¯æ•™ç¨‹",
        "ä¸Šæµ·æ–¹è¨€",
        "ä¸Šæµ·è¯å­¦ä¹ ",
        "æ²ªè¯­æ•™å­¦",
        "ä¸Šæµ·è¯å¯¹è¯",
        "ä¸Šæµ·è¯æ—¥å¸¸ç”¨è¯­",
        "ä¸Šæµ·è¯å‘éŸ³",
        "ä¸Šæµ·è¯è¯æ±‡",
        "ä¸Šæµ·è¯è¯­æ³•"
    ]
    
    # YouTubeã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    youtube_data = collector.collect_youtube_data(search_queries, max_duration_hours=5.0)
    
    # å­¦è¡“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    academic_data = collector.collect_academic_data()
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    all_data = youtube_data + academic_data
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    collector.save_collected_data(all_data)
    
    logger.info(f"ğŸ‰ ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: åˆè¨ˆ{len(all_data)}ä»¶")
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    total_duration = sum(data.duration for data in all_data)
    avg_quality = sum(data.quality_score for data in all_data) / len(all_data) if all_data else 0
    
    logger.info(f"ğŸ“Š çµ±è¨ˆæƒ…å ±:")
    logger.info(f"  - ç·æ™‚é–“: {total_duration/3600:.1f}æ™‚é–“")
    logger.info(f"  - å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.2f}")
    logger.info(f"  - è©±è€…æ•°: {len(set(data.speaker_id for data in all_data))}")

if __name__ == "__main__":
    import numpy as np
    main()
