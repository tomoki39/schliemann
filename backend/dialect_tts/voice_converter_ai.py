#!/usr/bin/env python3
"""
AIéŸ³å£°å¤‰æ›æŠ€è¡“ã«ã‚ˆã‚‹ä¸Šæµ·èªTTS
å®Ÿéš›ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¦æ–¹è¨€ã‚‰ã—ã„éŸ³å£°ã‚’ç”Ÿæˆ
"""

import os
import torch
import torchaudio
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
import logging
from typing import Optional, Tuple, Dict, Any
import json

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VoiceConverterAI:
    """AIéŸ³å£°å¤‰æ›å™¨ - æ¨™æº–ä¸­å›½èªã‹ã‚‰ä¸Šæµ·èªã¸å¤‰æ›"""
    
    def __init__(self, model_dir: str = "models/voice_converter"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sample_rate = 22050
        self.hop_length = 256
        self.win_length = 1024
        self.n_mel_channels = 80
        self.n_fft = 1024
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.encoder = None
        self.decoder = None
        self.vocoder = None
        self.speaker_encoder = None
        
        # ä¸Šæµ·èªç‰¹æœ‰ã®éŸ³éŸ»ç‰¹å¾´
        self.shanghai_phonetic_features = self._load_shanghai_features()
        
    def _load_shanghai_features(self) -> Dict[str, Any]:
        """ä¸Šæµ·èªç‰¹æœ‰ã®éŸ³éŸ»ç‰¹å¾´ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            # å­éŸ³ã®å¤‰åŒ–
            "consonant_changes": {
                "zh": "z",  # zh â†’ z
                "ch": "c",  # ch â†’ c  
                "sh": "s",  # sh â†’ s
                "r": "l",   # r â†’ l
                "n": "ng",  # ä¸€éƒ¨ã®n â†’ ng
            },
            # æ¯éŸ³ã®å¤‰åŒ–
            "vowel_changes": {
                "an": "ang",  # an â†’ ang
                "en": "eng",  # en â†’ eng
                "in": "ing",  # in â†’ ing
                "ian": "iang", # ian â†’ iang
            },
            # å£°èª¿ã®å¤‰åŒ–
            "tone_changes": {
                "tone1": 0.8,  # ç¬¬1å£°ã®ãƒ”ãƒƒãƒã‚’ä¸‹ã’ã‚‹
                "tone2": 1.2,  # ç¬¬2å£°ã®ãƒ”ãƒƒãƒã‚’ä¸Šã’ã‚‹
                "tone3": 0.9,  # ç¬¬3å£°ã®ãƒ”ãƒƒãƒã‚’ä¸‹ã’ã‚‹
                "tone4": 1.1,  # ç¬¬4å£°ã®ãƒ”ãƒƒãƒã‚’ä¸Šã’ã‚‹
            },
            # éŸ»å¾‹ã®ç‰¹å¾´
            "prosody": {
                "speaking_rate": 0.85,  # è©±é€Ÿã‚’é…ã
                "pitch_range": 0.7,     # ãƒ”ãƒƒãƒã®ç¯„å›²ã‚’ç‹­ã
                "energy_contour": 0.8,  # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å¤‰åŒ–ã‚’æŠ‘ãˆã‚‹
            }
        }
    
    def _create_dummy_models(self):
        """ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä»£ã‚ã‚Šï¼‰"""
        logger.info("ğŸ¤– ãƒ€ãƒŸãƒ¼ã®éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆéŸ³å£°ç‰¹å¾´æŠ½å‡ºï¼‰
        self.encoder = torch.nn.Sequential(
            torch.nn.Conv1d(1, 64, 15, 3, 7),
            torch.nn.ReLU(),
            torch.nn.Conv1d(64, 128, 5, 2, 2),
            torch.nn.ReLU(),
            torch.nn.Conv1d(128, 256, 3, 2, 1),
            torch.nn.ReLU(),
        ).to(self.device)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆéŸ³å£°ç‰¹å¾´å¤‰æ›ï¼‰
        self.decoder = torch.nn.Sequential(
            torch.nn.ConvTranspose1d(256, 128, 3, 2, 1, 1),
            torch.nn.ReLU(),
            torch.nn.ConvTranspose1d(128, 64, 5, 2, 2, 1),
            torch.nn.ReLU(),
            torch.nn.ConvTranspose1d(64, 1, 15, 3, 7, 0),
            torch.nn.Tanh(),
        ).to(self.device)
        
        # è©±è€…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆä¸Šæµ·èªè©±è€…ã®ç‰¹å¾´ã‚’å­¦ç¿’ï¼‰
        self.speaker_encoder = torch.nn.Sequential(
            torch.nn.Linear(256, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32),
        ).to(self.device)
        
        logger.info("âœ… ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆå®Œäº†")
    
    def _extract_audio_features(self, audio: np.ndarray) -> torch.Tensor:
        """éŸ³å£°ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¨ˆç®—
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=self.sample_rate,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            n_mels=self.n_mel_channels
        )
        
        # å¯¾æ•°å¤‰æ›
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        features = torch.FloatTensor(log_mel_spec).unsqueeze(0).to(self.device)
        
        return features
    
    def _apply_shanghai_phonetic_changes(self, features: torch.Tensor) -> torch.Tensor:
        """ä¸Šæµ·èªç‰¹æœ‰ã®éŸ³éŸ»å¤‰åŒ–ã‚’é©ç”¨"""
        # ç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼
        modified_features = features.clone()
        
        # å­éŸ³ã®å¤‰åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå‘¨æ³¢æ•°é ˜åŸŸã§ã®èª¿æ•´ï¼‰
        consonant_scale = 0.9  # å­éŸ³ã®å¼·åº¦ã‚’èª¿æ•´
        modified_features[:, :20, :] *= consonant_scale
        
        # æ¯éŸ³ã®å¤‰åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã®èª¿æ•´ï¼‰
        vowel_scale = 1.1  # æ¯éŸ³ã®ç‰¹æ€§ã‚’èª¿æ•´
        modified_features[:, 20:60, :] *= vowel_scale
        
        # å£°èª¿ã®å¤‰åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ”ãƒƒãƒã®èª¿æ•´ï¼‰
        tone_scale = 0.8  # å…¨ä½“çš„ãªãƒ”ãƒƒãƒã‚’ä¸‹ã’ã‚‹
        modified_features *= tone_scale
        
        return modified_features
    
    def _apply_shanghai_prosody(self, features: torch.Tensor) -> torch.Tensor:
        """ä¸Šæµ·èªç‰¹æœ‰ã®éŸ»å¾‹ã‚’é©ç”¨"""
        # è©±é€Ÿã®èª¿æ•´ï¼ˆæ™‚é–“è»¸ã®åœ§ç¸®ï¼‰
        prosody = self.shanghai_phonetic_features["prosody"]
        rate = prosody["speaking_rate"]
        
        # æ™‚é–“è»¸ã‚’åœ§ç¸®
        original_length = features.shape[-1]
        new_length = int(original_length * rate)
        
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if new_length != original_length:
            features_resampled = torch.nn.functional.interpolate(
                features.unsqueeze(0),
                size=new_length,
                mode='linear',
                align_corners=False
            ).squeeze(0)
        else:
            features_resampled = features
        
        # ãƒ”ãƒƒãƒç¯„å›²ã®èª¿æ•´
        pitch_range = prosody["pitch_range"]
        features_resampled *= pitch_range
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®èª¿æ•´
        energy_contour = prosody["energy_contour"]
        features_resampled *= energy_contour
        
        return features_resampled
    
    def convert_voice(self, audio: np.ndarray) -> np.ndarray:
        """éŸ³å£°ã‚’ä¸Šæµ·èªã«å¤‰æ›"""
        logger.info("ğŸµ éŸ³å£°ã‚’ä¸Šæµ·èªã«å¤‰æ›ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if self.encoder is None:
            self._create_dummy_models()
        
        # éŸ³å£°ã‚’æ­£è¦åŒ–
        audio = audio / np.max(np.abs(audio))
        
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_audio_features(audio)
        
        # ä¸Šæµ·èªã®éŸ³éŸ»å¤‰åŒ–ã‚’é©ç”¨
        shanghai_features = self._apply_shanghai_phonetic_changes(features)
        
        # ä¸Šæµ·èªã®éŸ»å¾‹ã‚’é©ç”¨
        prosody_features = self._apply_shanghai_prosody(shanghai_features)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        with torch.no_grad():
            encoded = self.encoder(prosody_features.unsqueeze(0))
            
            # è©±è€…ç‰¹å¾´ã‚’é©ç”¨
            speaker_features = self.speaker_encoder(encoded.mean(dim=-1))
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            converted = self.decoder(encoded)
            
        # éŸ³å£°ã«å¤‰æ›
        converted_audio = converted.squeeze().cpu().numpy()
        
        # æ­£è¦åŒ–
        converted_audio = converted_audio / np.max(np.abs(converted_audio))
        
        logger.info("âœ… éŸ³å£°å¤‰æ›å®Œäº†")
        return converted_audio
    
    def synthesize_shanghai_text(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸Šæµ·èªéŸ³å£°ã‚’åˆæˆ"""
        logger.info(f"ğŸ—£ï¸ ä¸Šæµ·èªãƒ†ã‚­ã‚¹ãƒˆåˆæˆ: {text}")
        
        # 1. èªå½™å¤‰æ›
        converted_text = self._convert_vocabulary(text)
        logger.info(f"èªå½™å¤‰æ›: {text} â†’ {converted_text}")
        
        # 2. åŸºæœ¬çš„ãªéŸ³å£°ç”Ÿæˆï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€TTSã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
        duration = len(converted_text) * 0.1  # æ–‡å­—æ•°ã«åŸºã¥ãæ¨å®šæ™‚é–“
        sample_rate = self.sample_rate
        samples = int(duration * sample_rate)
        
        # ã‚µã‚¤ãƒ³æ³¢ãƒ™ãƒ¼ã‚¹ã®ãƒ€ãƒŸãƒ¼éŸ³å£°
        t = np.linspace(0, duration, samples)
        base_freq = 200  # åŸºæœ¬å‘¨æ³¢æ•°
        
        # ä¸Šæµ·èªç‰¹æœ‰ã®ãƒ”ãƒƒãƒãƒ‘ã‚¿ãƒ¼ãƒ³
        pitch_pattern = np.sin(2 * np.pi * base_freq * t) * 0.5
        pitch_pattern += np.sin(2 * np.pi * base_freq * 1.5 * t) * 0.3
        pitch_pattern += np.sin(2 * np.pi * base_freq * 2 * t) * 0.2
        
        # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨
        envelope = np.exp(-t * 2)  # æ¸›è¡°ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—
        audio = pitch_pattern * envelope
        
        # 3. éŸ³å£°å¤‰æ›ã‚’é©ç”¨
        converted_audio = self.convert_voice(audio)
        
        return converted_audio
    
    def _convert_vocabulary(self, text: str) -> str:
        """èªå½™ã‚’ä¸Šæµ·èªã«å¤‰æ›"""
        vocabulary_map = {
            "ä½ å¥½": "ä¾¬å¥½",
            "ä»Šå¤©": "ä»Šæœ",
            "å¤©æ°”": "å¤©æ°”",
            "å¾ˆå¥½": "è›®å¥½",
            "æˆ‘ä»¬": "é˜¿æ‹‰",
            "ä»€ä¹ˆ": "å•¥",
            "æ€ä¹ˆ": "å“ªèƒ½",
            "å“ªé‡Œ": "å•¥åœ°æ–¹",
            "è°¢è°¢": "è°¢è°¢ä¾¬",
            "å†è§": "å†ä¼š",
            "åƒé¥­": "åšç”Ÿæ´»",
            "ç¡è§‰": "å›°è§‰",
            "å®¶": "å±‹é‡Œ",
            "å­¦æ ¡": "å­¦å ‚",
            "å·¥ä½œ": "åšç”Ÿæ´»",
            "å­¦ä¹ ": "è¯»ä¹¦",
            "æœ‹å‹": "æœ‹å‹",
            "è€å¸ˆ": "è€å¸ˆ",
            "å­¦ç”Ÿ": "å­¦ç”Ÿ",
            "åŒ»é™¢": "åŒ»é™¢",
            "å•†åº—": "å•†åº—",
            "é“¶è¡Œ": "é“¶è¡Œ",
            "è½¦ç«™": "è½¦ç«™",
            "æœºåœº": "æœºåœº",
        }
        
        converted = text
        for standard, shanghai in vocabulary_map.items():
            converted = converted.replace(standard, shanghai)
        
        return converted
    
    def save_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        model_path = Path(path)
        model_path.mkdir(parents=True, exist_ok=True)
        
        if self.encoder is not None:
            torch.save(self.encoder.state_dict(), model_path / "encoder.pth")
        if self.decoder is not None:
            torch.save(self.decoder.state_dict(), model_path / "decoder.pth")
        if self.speaker_encoder is not None:
            torch.save(self.speaker_encoder.state_dict(), model_path / "speaker_encoder.pth")
        
        # è¨­å®šã‚’ä¿å­˜
        config = {
            "sample_rate": self.sample_rate,
            "hop_length": self.hop_length,
            "win_length": self.win_length,
            "n_mel_channels": self.n_mel_channels,
            "n_fft": self.n_fft,
            "shanghai_phonetic_features": self.shanghai_phonetic_features
        }
        
        with open(model_path / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    
    def load_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        model_path = Path(path)
        
        if not model_path.exists():
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
            return False
        
        try:
            # è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
            with open(model_path / "config.json", 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
            self._create_dummy_models()
            
            # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
            if (model_path / "encoder.pth").exists():
                self.encoder.load_state_dict(torch.load(model_path / "encoder.pth", map_location=self.device))
            if (model_path / "decoder.pth").exists():
                self.decoder.load_state_dict(torch.load(model_path / "decoder.pth", map_location=self.device))
            if (model_path / "speaker_encoder.pth").exists():
                self.speaker_encoder.load_state_dict(torch.load(model_path / "speaker_encoder.pth", map_location=self.device))
            
            logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ AIéŸ³å£°å¤‰æ›ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹")
    
    # éŸ³å£°å¤‰æ›å™¨ã‚’åˆæœŸåŒ–
    converter = VoiceConverterAI()
    
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"
    
    # ä¸Šæµ·èªéŸ³å£°ã‚’åˆæˆ
    audio = converter.synthesize_shanghai_text(test_text)
    
    # éŸ³å£°ã‚’ä¿å­˜
    output_path = "test_output/shanghai_ai_converted.wav"
    os.makedirs("test_output", exist_ok=True)
    sf.write(output_path, audio, converter.sample_rate)
    
    logger.info(f"âœ… ä¸Šæµ·èªéŸ³å£°ã‚’ä¿å­˜: {output_path}")
    logger.info("ğŸ‰ AIéŸ³å£°å¤‰æ›ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    main()
