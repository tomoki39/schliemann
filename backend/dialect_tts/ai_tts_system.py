#!/usr/bin/env python3
"""
çµ±åˆAIéŸ³å£°å¤‰æ›TTSã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¦æœ¬æ ¼çš„ãªä¸Šæµ·èªTTSã‚’å®Ÿç¾
"""

import os
import torch
import numpy as np
import soundfile as sf
import librosa
from pathlib import Path
import logging
from typing import Optional, Dict, Any
import json
import asyncio
from voice_converter_ai import VoiceConverterAI
from train_voice_converter import VoiceConverterModel, VoiceConverterTrainer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AITTSystem:
    """çµ±åˆAIéŸ³å£°å¤‰æ›TTSã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_dir: str = "models/ai_tts"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # éŸ³å£°å¤‰æ›å™¨
        self.voice_converter = VoiceConverterAI(str(self.model_dir / "voice_converter"))
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        self.trained_model = None
        self.is_model_loaded = False
        
        # éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sample_rate = 22050
        
        # ä¸Šæµ·èªèªå½™ãƒãƒƒãƒ—
        self.vocabulary_map = self._load_vocabulary_map()
        
    def _load_vocabulary_map(self) -> Dict[str, str]:
        """ä¸Šæµ·èªèªå½™ãƒãƒƒãƒ—ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            # åŸºæœ¬æŒ¨æ‹¶
            "ä½ å¥½": "ä¾¬å¥½",
            "æ‚¨å¥½": "ä¾¬å¥½",
            "æ—©ä¸Šå¥½": "æ—©æµªå¥½",
            "æ™šä¸Šå¥½": "å¤œåˆ°å¥½",
            "å†è§": "å†ä¼š",
            "è°¢è°¢": "è°¢è°¢ä¾¬",
            "ä¸å®¢æ°”": "å‹¿è¦å®¢æ°”",
            "å¯¹ä¸èµ·": "å¯¹å‹¿èµ·",
            "æ²¡å…³ç³»": "å‹¿è¦ç´§",
            
            # æ™‚é–“ãƒ»æ—¥ä»˜
            "ä»Šå¤©": "ä»Šæœ",
            "æ˜å¤©": "æ˜æœ",
            "æ˜¨å¤©": "æ˜¨æ—¥å­",
            "ç°åœ¨": "ç°åœ¨",
            "åˆšæ‰": "åˆšåˆš",
            "ä¸€ä¼šå„¿": "ä¸€æ­‡æ­‡",
            
            # å¤©æ°—ãƒ»è‡ªç„¶
            "å¤©æ°”": "å¤©æ°”",
            "å¾ˆå¥½": "è›®å¥½",
            "ä¸é”™": "å‹¿é”™",
            "ä¸‹é›¨": "è½é›¨",
            "æ™´å¤©": "æ™´å¤©",
            "é˜´å¤©": "é˜´å¤©",
            
            # å®¶æ—ãƒ»äºº
            "æˆ‘ä»¬": "é˜¿æ‹‰",
            "ä½ ä»¬": "ä¾¬æ‹‰",
            "ä»–ä»¬": "ä¼Šæ‹‰",
            "æˆ‘": "æˆ‘",
            "ä½ ": "ä¾¬",
            "ä»–": "ä¼Š",
            "å¥¹": "ä¼Š",
            "æœ‹å‹": "æœ‹å‹",
            "è€å¸ˆ": "è€å¸ˆ",
            "å­¦ç”Ÿ": "å­¦ç”Ÿ",
            
            # å ´æ‰€
            "å®¶": "å±‹é‡Œ",
            "å­¦æ ¡": "å­¦å ‚",
            "åŒ»é™¢": "åŒ»é™¢",
            "å•†åº—": "å•†åº—",
            "é“¶è¡Œ": "é“¶è¡Œ",
            "è½¦ç«™": "è½¦ç«™",
            "æœºåœº": "æœºåœº",
            "å“ªé‡Œ": "å•¥åœ°æ–¹",
            "è¿™é‡Œ": "æ¿æ­",
            "é‚£é‡Œ": "ä¼Šæ­",
            
            # å‹•ä½œãƒ»å‹•è©
            "å»": "å»",
            "æ¥": "æ¥",
            "åƒ": "åƒ",
            "å–": "åƒ",
            "ç¡": "å›°",
            "å·¥ä½œ": "åšç”Ÿæ´»",
            "å­¦ä¹ ": "è¯»ä¹¦",
            "çœ‹": "çœ‹",
            "å¬": "å¬",
            "è¯´": "è®²",
            "çŸ¥é“": "æ™“å¾—",
            "ä¸çŸ¥é“": "å‹¿æ™“å¾—",
            "è¦": "è¦",
            "ä¸è¦": "å‹¿è¦",
            "å¯ä»¥": "å¯ä»¥",
            "ä¸å¯ä»¥": "å‹¿å¯ä»¥",
            
            # ç–‘å•è©
            "ä»€ä¹ˆ": "å•¥",
            "æ€ä¹ˆ": "å“ªèƒ½",
            "ä¸ºä»€ä¹ˆ": "ä¸ºå•¥",
            "ä»€ä¹ˆæ—¶å€™": "å•¥è¾°å…‰",
            "å¤šå°‘": "å‡ åŒ–",
            "å‡ ä¸ª": "å‡ ä¸ª",
            
            # å½¢å®¹è©
            "å¤§": "å¤§",
            "å°": "å°",
            "å¥½": "å¥½",
            "å": "å",
            "æ–°": "æ–°",
            "æ—§": "æ—§",
            "å¿«": "å¿«",
            "æ…¢": "æ…¢",
            "çƒ­": "çƒ­",
            "å†·": "å†·",
            "é«˜": "é«˜",
            "ä½": "ä½",
            "é•¿": "é•¿",
            "çŸ­": "çŸ­",
            
            # æ•°è©
            "ä¸€": "ä¸€",
            "äºŒ": "ä¸¤",
            "ä¸‰": "ä¸‰",
            "å››": "å››",
            "äº”": "äº”",
            "å…­": "å…­",
            "ä¸ƒ": "ä¸ƒ",
            "å…«": "å…«",
            "ä¹": "ä¹",
            "å": "å",
        }
    
    def convert_text_to_shanghai(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸Šæµ·èªã«å¤‰æ›"""
        converted = text
        
        # èªå½™å¤‰æ›ã‚’é©ç”¨
        for standard, shanghai in self.vocabulary_map.items():
            converted = converted.replace(standard, shanghai)
        
        return converted
    
    def generate_base_audio(self, text: str) -> np.ndarray:
        """åŸºæœ¬çš„ãªéŸ³å£°ã‚’ç”Ÿæˆï¼ˆTTSã‚¨ãƒ³ã‚¸ãƒ³ã®ä»£ã‚ã‚Šï¼‰"""
        # æ–‡å­—æ•°ã«åŸºã¥ãæ¨å®šæ™‚é–“
        duration = len(text) * 0.15  # 1æ–‡å­—ã‚ãŸã‚Š0.15ç§’
        samples = int(duration * self.sample_rate)
        
        # åŸºæœ¬å‘¨æ³¢æ•°ï¼ˆç”·æ€§ã®å£°ã‚’æƒ³å®šï¼‰
        base_freq = 150
        
        # æ™‚é–“è»¸
        t = np.linspace(0, duration, samples)
        
        # è¤‡æ•°ã®å‘¨æ³¢æ•°æˆåˆ†ã‚’åˆæˆ
        audio = np.zeros(samples)
        
        # åŸºæœ¬éŸ³
        audio += np.sin(2 * np.pi * base_freq * t) * 0.6
        
        # å€éŸ³
        audio += np.sin(2 * np.pi * base_freq * 2 * t) * 0.3
        audio += np.sin(2 * np.pi * base_freq * 3 * t) * 0.1
        
        # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨ï¼ˆè‡ªç„¶ãªæ¸›è¡°ï¼‰
        envelope = np.exp(-t * 1.5)
        audio *= envelope
        
        # ãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼ˆã‚ˆã‚Šè‡ªç„¶ã«ï¼‰
        noise = np.random.normal(0, 0.05, samples)
        audio += noise
        
        # æ­£è¦åŒ–
        audio = audio / np.max(np.abs(audio))
        
        return audio
    
    def apply_shanghai_phonetic_changes(self, audio: np.ndarray) -> np.ndarray:
        """ä¸Šæµ·èªç‰¹æœ‰ã®éŸ³éŸ»å¤‰åŒ–ã‚’é©ç”¨"""
        # 1. ãƒ”ãƒƒãƒã®èª¿æ•´ï¼ˆå…¨ä½“çš„ã«ä½ãï¼‰
        pitch_scale = 0.9
        audio = librosa.effects.pitch_shift(audio, sr=self.sample_rate, n_steps=-2)
        
        # 2. è©±é€Ÿã®èª¿æ•´ï¼ˆå°‘ã—é…ãï¼‰
        rate_scale = 0.85
        audio = librosa.effects.time_stretch(audio, rate=rate_scale)
        
        # 3. ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã®èª¿æ•´ï¼ˆä¸Šæµ·èªç‰¹æœ‰ã®éŸ³è‰²ï¼‰
        # é«˜å‘¨æ³¢æˆåˆ†ã‚’å¼·èª¿
        fft = np.fft.fft(audio)
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        
        # 2000-4000Hzã®æˆåˆ†ã‚’å¼·èª¿
        mask = (freqs >= 2000) & (freqs <= 4000)
        fft[mask] *= 1.2
        
        audio = np.real(np.fft.ifft(fft))
        
        # 4. éŸ»å¾‹ã®èª¿æ•´
        # éŸ³ç¯€ã®å¢ƒç•Œã§å°‘ã—é–“ã‚’ç©ºã‘ã‚‹
        chunk_size = len(audio) // 10  # 10å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        processed_audio = []
        
        for i in range(0, len(audio), chunk_size):
            chunk = audio[i:i+chunk_size]
            processed_audio.extend(chunk)
            
            # ãƒãƒ£ãƒ³ã‚¯ã®é–“ã«çŸ­ã„ç„¡éŸ³ã‚’æŒ¿å…¥
            if i + chunk_size < len(audio):
                silence = np.zeros(int(0.05 * self.sample_rate))  # 0.05ç§’ã®ç„¡éŸ³
                processed_audio.extend(silence)
        
        return np.array(processed_audio)
    
    async def synthesize_shanghai_audio(self, text: str) -> np.ndarray:
        """ä¸Šæµ·èªéŸ³å£°ã‚’åˆæˆ"""
        logger.info(f"ğŸ—£ï¸ ä¸Šæµ·èªéŸ³å£°åˆæˆé–‹å§‹: {text}")
        
        # 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸Šæµ·èªã«å¤‰æ›
        shanghai_text = self.convert_text_to_shanghai(text)
        logger.info(f"èªå½™å¤‰æ›: {text} â†’ {shanghai_text}")
        
        # 2. åŸºæœ¬çš„ãªéŸ³å£°ã‚’ç”Ÿæˆ
        base_audio = self.generate_base_audio(shanghai_text)
        
        # 3. ä¸Šæµ·èªç‰¹æœ‰ã®éŸ³éŸ»å¤‰åŒ–ã‚’é©ç”¨
        phonetic_audio = self.apply_shanghai_phonetic_changes(base_audio)
        
        # 4. AIéŸ³å£°å¤‰æ›ã‚’é©ç”¨ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
        if self.is_model_loaded:
            try:
                converted_audio = self.voice_converter.convert_voice(phonetic_audio)
                final_audio = converted_audio
                logger.info("âœ… AIéŸ³å£°å¤‰æ›ã‚’é©ç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸ AIéŸ³å£°å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                final_audio = phonetic_audio
        else:
            final_audio = phonetic_audio
            logger.info("â„¹ï¸ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãªã—ã€åŸºæœ¬å¤‰æ›ã®ã¿é©ç”¨")
        
        # 5. æœ€çµ‚çš„ãªæ­£è¦åŒ–
        final_audio = final_audio / np.max(np.abs(final_audio))
        
        logger.info("âœ… ä¸Šæµ·èªéŸ³å£°åˆæˆå®Œäº†")
        return final_audio
    
    def load_trained_model(self, model_path: str) -> bool:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            model_file = Path(model_path) / "voice_converter.pth"
            if model_file.exists():
                # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                self.trained_model = VoiceConverterModel()
                self.trained_model.load_state_dict(torch.load(model_file, map_location=self.device))
                self.trained_model.to(self.device)
                self.trained_model.eval()
                
                self.is_model_loaded = True
                logger.info(f"âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_path}")
                return True
            else:
                logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
                return False
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_audio(self, audio: np.ndarray, filename: str) -> str:
        """éŸ³å£°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_path = self.model_dir / "output" / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        sf.write(str(output_path), audio, self.sample_rate)
        logger.info(f"âœ… éŸ³å£°ã‚’ä¿å­˜: {output_path}")
        
        return str(output_path)
    
    def get_audio_info(self, audio: np.ndarray) -> Dict[str, Any]:
        """éŸ³å£°ã®æƒ…å ±ã‚’å–å¾—"""
        duration = len(audio) / self.sample_rate
        
        # åŸºæœ¬çµ±è¨ˆ
        rms = np.sqrt(np.mean(audio**2))
        peak = np.max(np.abs(audio))
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ç‰¹å¾´
        fft = np.fft.fft(audio)
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        magnitude = np.abs(fft)
        
        # ä¸»è¦å‘¨æ³¢æ•°
        positive_freqs = freqs[:len(freqs)//2]
        positive_magnitude = magnitude[:len(magnitude)//2]
        dominant_freq = positive_freqs[np.argmax(positive_magnitude)]
        
        return {
            "duration": duration,
            "sample_rate": self.sample_rate,
            "rms": rms,
            "peak": peak,
            "dominant_frequency": dominant_freq,
            "length_samples": len(audio)
        }

async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ çµ±åˆAIéŸ³å£°å¤‰æ›TTSã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")
    
    # TTSã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    tts_system = AITTSystem()
    
    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ã‚Œã°ï¼‰
    model_loaded = tts_system.load_trained_model("models/voice_converter_trained")
    
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_texts = [
        "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚",
        "æˆ‘ä»¬ä¸€èµ·å»åƒé¥­å§ã€‚",
        "è°¢è°¢ä½ çš„å¸®åŠ©ã€‚",
        "å­¦ä¹ ä¸Šæµ·è¯å¾ˆæœ‰è¶£ã€‚",
        "å†è§ï¼Œæ˜å¤©è§ã€‚"
    ]
    
    # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆæˆ
    for i, text in enumerate(test_texts):
        logger.info(f"\n--- ãƒ†ã‚¹ãƒˆ {i+1}: {text} ---")
        
        # ä¸Šæµ·èªéŸ³å£°ã‚’åˆæˆ
        audio = await tts_system.synthesize_shanghai_audio(text)
        
        # éŸ³å£°æƒ…å ±ã‚’è¡¨ç¤º
        info = tts_system.get_audio_info(audio)
        logger.info(f"éŸ³å£°æƒ…å ±: é•·ã•={info['duration']:.2f}ç§’, "
                   f"RMS={info['rms']:.3f}, "
                   f"ä¸»è¦å‘¨æ³¢æ•°={info['dominant_frequency']:.1f}Hz")
        
        # éŸ³å£°ã‚’ä¿å­˜
        filename = f"shanghai_ai_{i+1:02d}.wav"
        output_path = tts_system.save_audio(audio, filename)
        
        logger.info(f"ä¿å­˜å®Œäº†: {output_path}")
    
    logger.info("\nğŸ‰ çµ±åˆAIéŸ³å£°å¤‰æ›TTSãƒ†ã‚¹ãƒˆå®Œäº†")
    logger.info("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ models/ai_tts/output/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    asyncio.run(main())
