#!/usr/bin/env python3
"""
éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸Šæµ·èªã®ç‰¹å¾´ã‚’å­¦ç¿’
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
import logging
from typing import List, Tuple, Dict, Any
import json
from dataclasses import dataclass
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AudioPair:
    """éŸ³å£°ãƒšã‚¢ï¼ˆæ¨™æº–ä¸­å›½èªã¨ä¸Šæµ·èªï¼‰"""
    standard_audio: np.ndarray
    shanghai_audio: np.ndarray
    text: str
    duration: float

class VoiceConverterDataset(Dataset):
    """éŸ³å£°å¤‰æ›ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, audio_pairs: List[AudioPair], sample_rate: int = 22050):
        self.audio_pairs = audio_pairs
        self.sample_rate = sample_rate
        
    def __len__(self):
        return len(self.audio_pairs)
    
    def __getitem__(self, idx):
        pair = self.audio_pairs[idx]
        
        # éŸ³å£°ã‚’æ­£è¦åŒ–
        standard = pair.standard_audio / np.max(np.abs(pair.standard_audio))
        shanghai = pair.shanghai_audio / np.max(np.abs(pair.shanghai_audio))
        
        # é•·ã•ã‚’æƒãˆã‚‹ï¼ˆçŸ­ã„æ–¹ã«åˆã‚ã›ã‚‹ï¼‰
        min_length = min(len(standard), len(shanghai))
        standard = standard[:min_length]
        shanghai = shanghai[:min_length]
        
        return {
            'standard': torch.FloatTensor(standard),
            'shanghai': torch.FloatTensor(shanghai),
            'text': pair.text,
            'duration': pair.duration
        }

class VoiceConverterModel(nn.Module):
    """éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, input_dim: int = 80, hidden_dim: int = 256, output_dim: int = 80):
        super().__init__()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.encoder = nn.Sequential(
            nn.Conv1d(input_dim, 128, 15, 3, 7),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Conv1d(128, 256, 5, 2, 2),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Conv1d(256, 512, 3, 2, 1),
            nn.BatchNorm1d(512),
            nn.ReLU(),
        )
        
        # å¤‰æ›å±¤
        self.converter = nn.Sequential(
            nn.Linear(512, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 512),
            nn.ReLU(),
        )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(512, 256, 3, 2, 1, 1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.ConvTranspose1d(256, 128, 5, 2, 2, 1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.ConvTranspose1d(128, output_dim, 15, 3, 7, 0),
            nn.Tanh(),
        )
        
        # è©±è€…è­˜åˆ¥å™¨ï¼ˆæ•µå¯¾çš„å­¦ç¿’ç”¨ï¼‰
        self.speaker_discriminator = nn.Sequential(
            nn.Conv1d(output_dim, 64, 15, 3, 7),
            nn.LeakyReLU(0.2),
            nn.Conv1d(64, 128, 5, 2, 2),
            nn.LeakyReLU(0.2),
            nn.Conv1d(128, 256, 3, 2, 1),
            nn.LeakyReLU(0.2),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded = self.encoder(x)
        
        # å¤‰æ›
        batch_size, channels, length = encoded.shape
        encoded_flat = encoded.view(batch_size, channels, -1).transpose(1, 2)
        converted_flat = self.converter(encoded_flat)
        converted = converted_flat.transpose(1, 2).view(batch_size, 512, length)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        output = self.decoder(converted)
        
        return output
    
    def discriminate_speaker(self, x):
        """è©±è€…ã‚’è­˜åˆ¥"""
        return self.speaker_discriminator(x)

class VoiceConverterTrainer:
    """éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å™¨"""
    
    def __init__(self, model: VoiceConverterModel, device: str = "cpu"):
        self.model = model.to(device)
        self.device = device
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))
        self.discriminator_optimizer = optim.Adam(
            self.model.speaker_discriminator.parameters(), 
            lr=0.0001, betas=(0.5, 0.999)
        )
        
        # æå¤±é–¢æ•°
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()
        
        # å­¦ç¿’å±¥æ­´
        self.train_losses = []
        self.discriminator_losses = []
        
    def extract_features(self, audio: np.ndarray) -> torch.Tensor:
        """éŸ³å£°ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¨ˆç®—
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=22050,
            n_fft=1024,
            hop_length=256,
            win_length=1024,
            n_mels=80
        )
        
        # å¯¾æ•°å¤‰æ›
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        return torch.FloatTensor(log_mel_spec).unsqueeze(0)
    
    def train_step(self, standard_features: torch.Tensor, shanghai_features: torch.Tensor):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        standard_features = standard_features.to(self.device)
        shanghai_features = shanghai_features.to(self.device)
        
        # ç”Ÿæˆå™¨ã®å­¦ç¿’
        self.optimizer.zero_grad()
        
        # éŸ³å£°å¤‰æ›
        converted_features = self.model(standard_features)
        
        # å†æ§‹æˆæå¤±
        recon_loss = self.mse_loss(converted_features, shanghai_features)
        
        # æ•µå¯¾çš„æå¤±
        fake_pred = self.model.discriminate_speaker(converted_features)
        real_labels = torch.ones_like(fake_pred)
        adv_loss = self.bce_loss(fake_pred, real_labels)
        
        # ç·æå¤±
        generator_loss = recon_loss + 0.1 * adv_loss
        
        generator_loss.backward()
        self.optimizer.step()
        
        # è­˜åˆ¥å™¨ã®å­¦ç¿’
        self.discriminator_optimizer.zero_grad()
        
        # æœ¬ç‰©ã®éŸ³å£°
        real_pred = self.model.discriminate_speaker(shanghai_features)
        real_labels = torch.ones_like(real_pred)
        real_loss = self.bce_loss(real_pred, real_labels)
        
        # å½ç‰©ã®éŸ³å£°
        fake_pred = self.model.discriminate_speaker(converted_features.detach())
        fake_labels = torch.zeros_like(fake_pred)
        fake_loss = self.bce_loss(fake_pred, fake_labels)
        
        discriminator_loss = (real_loss + fake_loss) / 2
        discriminator_loss.backward()
        self.discriminator_optimizer.step()
        
        return {
            'generator_loss': generator_loss.item(),
            'discriminator_loss': discriminator_loss.item(),
            'recon_loss': recon_loss.item(),
            'adv_loss': adv_loss.item()
        }
    
    def train(self, dataloader: DataLoader, epochs: int = 100):
        """å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ å­¦ç¿’ã‚’é–‹å§‹: {epochs}ã‚¨ãƒãƒƒã‚¯")
        
        for epoch in range(epochs):
            epoch_generator_loss = 0
            epoch_discriminator_loss = 0
            num_batches = 0
            
            for batch in dataloader:
                standard_audio = batch['standard']
                shanghai_audio = batch['shanghai']
                
                # ç‰¹å¾´é‡æŠ½å‡º
                standard_features = torch.stack([
                    self.extract_features(audio.numpy()) for audio in standard_audio
                ]).squeeze(1)
                
                shanghai_features = torch.stack([
                    self.extract_features(audio.numpy()) for audio in shanghai_audio
                ]).squeeze(1)
                
                # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
                losses = self.train_step(standard_features, shanghai_features)
                
                epoch_generator_loss += losses['generator_loss']
                epoch_discriminator_loss += losses['discriminator_loss']
                num_batches += 1
            
            # ã‚¨ãƒãƒƒã‚¯å¹³å‡æå¤±
            avg_generator_loss = epoch_generator_loss / num_batches
            avg_discriminator_loss = epoch_discriminator_loss / num_batches
            
            self.train_losses.append(avg_generator_loss)
            self.discriminator_losses.append(avg_discriminator_loss)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{epochs}: "
                          f"Generator Loss: {avg_generator_loss:.4f}, "
                          f"Discriminator Loss: {avg_discriminator_loss:.4f}")
        
        logger.info("âœ… å­¦ç¿’å®Œäº†")
    
    def save_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        model_path = Path(path)
        model_path.mkdir(parents=True, exist_ok=True)
        
        torch.save(self.model.state_dict(), model_path / "voice_converter.pth")
        
        # å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜
        history = {
            'train_losses': self.train_losses,
            'discriminator_losses': self.discriminator_losses
        }
        
        with open(model_path / "training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
        
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    
    def plot_training_history(self, save_path: str = "training_history.png"):
        """å­¦ç¿’å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Generator Loss')
        plt.plot(self.discriminator_losses, label='Discriminator Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('Training Losses')
        
        plt.subplot(1, 2, 2)
        plt.plot(self.train_losses, label='Generator Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('Generator Loss Detail')
        
        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
        
        logger.info(f"âœ… å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜: {save_path}")

def create_dummy_dataset() -> List[AudioPair]:
    """ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    logger.info("ğŸ“š ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
    
    audio_pairs = []
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®éŸ³å£°ãƒšã‚¢ã‚’ç”Ÿæˆ
    test_texts = [
        "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚",
        "æˆ‘ä»¬ä¸€èµ·å»åƒé¥­å§ã€‚",
        "è°¢è°¢ä½ çš„å¸®åŠ©ã€‚",
        "å†è§ï¼Œæ˜å¤©è§ã€‚",
        "å­¦ä¹ ä¸Šæµ·è¯å¾ˆæœ‰è¶£ã€‚"
    ]
    
    for i, text in enumerate(test_texts):
        # ãƒ€ãƒŸãƒ¼ã®æ¨™æº–ä¸­å›½èªéŸ³å£°
        duration = 2.0
        sample_rate = 22050
        samples = int(duration * sample_rate)
        
        t = np.linspace(0, duration, samples)
        standard_audio = np.sin(2 * np.pi * 200 * t) * 0.5  # 200Hz
        
        # ãƒ€ãƒŸãƒ¼ã®ä¸Šæµ·èªéŸ³å£°ï¼ˆå°‘ã—ç•°ãªã‚‹å‘¨æ³¢æ•°ï¼‰
        shanghai_audio = np.sin(2 * np.pi * 180 * t) * 0.6  # 180Hz
        
        # ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ã‚ˆã‚Šç¾å®Ÿçš„ã«
        standard_audio += np.random.normal(0, 0.1, samples)
        shanghai_audio += np.random.normal(0, 0.1, samples)
        
        pair = AudioPair(
            standard_audio=standard_audio,
            shanghai_audio=shanghai_audio,
            text=text,
            duration=duration
        )
        
        audio_pairs.append(pair)
    
    logger.info(f"âœ… ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(audio_pairs)}ä»¶")
    return audio_pairs

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("ğŸš€ AIéŸ³å£°å¤‰æ›å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    audio_pairs = create_dummy_dataset()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    dataset = VoiceConverterDataset(audio_pairs)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = VoiceConverterModel()
    
    # å­¦ç¿’å™¨ã‚’ä½œæˆ
    trainer = VoiceConverterTrainer(model, device)
    
    # å­¦ç¿’ã‚’å®Ÿè¡Œ
    trainer.train(dataloader, epochs=50)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    trainer.save_model("models/voice_converter_trained")
    
    # å­¦ç¿’å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    trainer.plot_training_history("training_history.png")
    
    logger.info("ğŸ‰ AIéŸ³å£°å¤‰æ›å­¦ç¿’å®Œäº†")

if __name__ == "__main__":
    main()
