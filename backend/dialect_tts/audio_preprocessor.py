#!/usr/bin/env python3
"""
éŸ³å£°å‰å‡¦ç†ãƒ»å“è³ªå‘ä¸Šã‚·ã‚¹ãƒ†ãƒ 
åé›†ã—ãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é«˜å“è³ªã«å‰å‡¦ç†ã—ã€æ–¹è¨€ç‰¹å¾´ã‚’æŠ½å‡º
"""

import os
import numpy as np
import librosa
import soundfile as sf
import noisereduce as nr
from scipy import signal
from scipy.signal import butter, filtfilt
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProcessedAudio:
    """å‰å‡¦ç†æ¸ˆã¿éŸ³å£°ãƒ‡ãƒ¼ã‚¿"""
    file_path: str
    original_path: str
    duration: float
    sample_rate: int
    quality_score: float
    phonetic_features: Dict
    dialect_features: Dict
    speaker_embedding: np.ndarray

class AudioPreprocessor:
    """éŸ³å£°å‰å‡¦ç†å™¨"""
    
    def __init__(self, output_dir: str = "processed_audio"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # éŸ³å£°å“è³ªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.target_sample_rate = 22050
        self.min_duration = 1.0  # æœ€å°1ç§’
        self.max_duration = 30.0  # æœ€å¤§30ç§’
        self.min_quality_score = 0.4
        
    def process_audio_collection(self, metadata_file: str) -> List[ProcessedAudio]:
        """éŸ³å£°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’å‰å‡¦ç†"""
        logger.info("ğŸµ éŸ³å£°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å‰å‡¦ç†ã‚’é–‹å§‹")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        processed_audio_list = []
        
        for i, audio_meta in enumerate(metadata):
            logger.info(f"ğŸ”„ å‡¦ç†ä¸­ ({i+1}/{len(metadata)}): {audio_meta['file_path']}")
            
            try:
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†
                processed_audio = self.process_single_audio(audio_meta)
                
                if processed_audio:
                    processed_audio_list.append(processed_audio)
                    logger.info(f"âœ… å‡¦ç†å®Œäº†: {processed_audio.file_path}")
                else:
                    logger.warning(f"âš ï¸ å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {audio_meta['file_path']}")
                    
            except Exception as e:
                logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ğŸ‰ å‰å‡¦ç†å®Œäº†: {len(processed_audio_list)}ä»¶")
        return processed_audio_list
    
    def process_single_audio(self, audio_meta: Dict) -> Optional[ProcessedAudio]:
        """å˜ä¸€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†"""
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            audio, sample_rate = librosa.load(audio_meta['file_path'], sr=None)
            
            # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
            if len(audio) == 0:
                return None
            
            duration = len(audio) / sample_rate
            if duration < self.min_duration or duration > self.max_duration:
                logger.warning(f"âš ï¸ é•·ã•ãŒä¸é©åˆ‡: {duration:.1f}s")
                return None
            
            # éŸ³å£°å“è³ªã‚’è©•ä¾¡
            quality_score = self.evaluate_audio_quality(audio, sample_rate)
            if quality_score < self.min_quality_score:
                logger.warning(f"âš ï¸ å“è³ªãŒä½ã„: {quality_score:.2f}")
                return None
            
            # éŸ³å£°å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
            processed_audio = self.apply_preprocessing_pipeline(
                audio, sample_rate, audio_meta
            )
            
            if processed_audio is None:
                return None
            
            # éŸ³éŸ»ç‰¹å¾´ã‚’æŠ½å‡º
            phonetic_features = self.extract_phonetic_features(processed_audio)
            
            # æ–¹è¨€ç‰¹å¾´ã‚’æŠ½å‡º
            dialect_features = self.extract_dialect_features(processed_audio)
            
            # è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            speaker_embedding = self.extract_speaker_embedding(processed_audio)
            
            # æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            output_file = self.save_processed_audio(processed_audio, audio_meta)
            
            return ProcessedAudio(
                file_path=output_file,
                original_path=audio_meta['file_path'],
                duration=len(processed_audio) / self.target_sample_rate,
                sample_rate=self.target_sample_rate,
                quality_score=quality_score,
                phonetic_features=phonetic_features,
                dialect_features=dialect_features,
                speaker_embedding=speaker_embedding
            )
            
        except Exception as e:
            logger.error(f"âŒ å˜ä¸€éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def apply_preprocessing_pipeline(self, audio: np.ndarray, sample_rate: int, metadata: Dict) -> Optional[np.ndarray]:
        """éŸ³å£°å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é©ç”¨"""
        try:
            # 1. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’çµ±ä¸€
            if sample_rate != self.target_sample_rate:
                audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=self.target_sample_rate)
            
            # 2. ãƒã‚¤ã‚ºé™¤å»
            audio = self.reduce_noise(audio)
            
            # 3. éŸ³é‡æ­£è¦åŒ–
            audio = self.normalize_volume(audio)
            
            # 4. é«˜å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»
            audio = self.remove_high_frequency_noise(audio)
            
            # 5. éŸ³å£°ã®åˆ‡ã‚Šå‡ºã—ï¼ˆç„¡éŸ³éƒ¨åˆ†ã‚’é™¤å»ï¼‰
            audio = self.trim_silence(audio)
            
            # 6. æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯
            if len(audio) == 0:
                return None
            
            return audio
            
        except Exception as e:
            logger.error(f"âŒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def reduce_noise(self, audio: np.ndarray) -> np.ndarray:
        """ãƒã‚¤ã‚ºé™¤å»"""
        try:
            # noisereduceãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
            reduced_noise = nr.reduce_noise(y=audio, sr=self.target_sample_rate)
            return reduced_noise
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒã‚¤ã‚ºé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def normalize_volume(self, audio: np.ndarray) -> np.ndarray:
        """éŸ³é‡æ­£è¦åŒ–"""
        try:
            # RMSæ­£è¦åŒ–
            rms = np.sqrt(np.mean(audio**2))
            if rms > 0:
                audio = audio / rms * 0.1  # é©åˆ‡ãªéŸ³é‡ãƒ¬ãƒ™ãƒ«ã«èª¿æ•´
            return audio
        except Exception as e:
            logger.warning(f"âš ï¸ éŸ³é‡æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def remove_high_frequency_noise(self, audio: np.ndarray) -> np.ndarray:
        """é«˜å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»"""
        try:
            # ãƒã‚¿ãƒ¼ãƒ¯ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
            nyquist = self.target_sample_rate / 2
            cutoff = 8000  # 8kHzä»¥ä¸‹ã‚’ä¿æŒ
            normal_cutoff = cutoff / nyquist
            b, a = butter(4, normal_cutoff, btype='low', analog=False)
            filtered_audio = filtfilt(b, a, audio)
            return filtered_audio
        except Exception as e:
            logger.warning(f"âš ï¸ é«˜å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def trim_silence(self, audio: np.ndarray, threshold: float = 0.01) -> np.ndarray:
        """ç„¡éŸ³éƒ¨åˆ†ã‚’é™¤å»"""
        try:
            # ç„¡éŸ³éƒ¨åˆ†ã‚’æ¤œå‡º
            frame_length = int(0.025 * self.target_sample_rate)  # 25ms
            hop_length = int(0.010 * self.target_sample_rate)    # 10ms
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã§ç„¡éŸ³ã‚’æ¤œå‡º
            energy = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]
            
            # é–¾å€¤ä»¥ä¸‹ã®éƒ¨åˆ†ã‚’ç„¡éŸ³ã¨ã—ã¦æ¤œå‡º
            silence_frames = energy < threshold
            
            # ç„¡éŸ³éƒ¨åˆ†ã‚’é™¤å»
            if not silence_frames.all():
                # æœ€åˆã¨æœ€å¾Œã®ç„¡éŸ³éƒ¨åˆ†ã‚’é™¤å»
                start_frame = 0
                end_frame = len(silence_frames)
                
                for i, is_silent in enumerate(silence_frames):
                    if not is_silent:
                        start_frame = i
                        break
                
                for i in range(len(silence_frames) - 1, -1, -1):
                    if not silence_frames[i]:
                        end_frame = i + 1
                        break
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µãƒ³ãƒ—ãƒ«ã«å¤‰æ›
                start_sample = start_frame * hop_length
                end_sample = end_frame * hop_length
                
                # éŸ³å£°ã‚’åˆ‡ã‚Šå‡ºã—
                trimmed_audio = audio[start_sample:end_sample]
                
                return trimmed_audio
            else:
                return audio
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡éŸ³é™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def evaluate_audio_quality(self, audio: np.ndarray, sample_rate: int) -> float:
        """éŸ³å£°å“è³ªã‚’è©•ä¾¡"""
        try:
            # 1. ä¿¡å·å¯¾é›‘éŸ³æ¯”
            signal_power = np.mean(audio ** 2)
            noise_power = np.var(audio - np.mean(audio))
            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            
            # 2. å‘¨æ³¢æ•°ç‰¹æ€§
            freqs = np.fft.fftfreq(len(audio), 1/sample_rate)
            fft = np.abs(np.fft.fft(audio))
            
            # äººé–“ã®éŸ³å£°å‘¨æ³¢æ•°ç¯„å›²ï¼ˆ80Hz-8000Hzï¼‰ã§ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼
            voice_freq_mask = (freqs >= 80) & (freqs <= 8000)
            voice_energy = np.sum(fft[voice_freq_mask])
            total_energy = np.sum(fft)
            voice_ratio = voice_energy / (total_energy + 1e-10)
            
            # 3. å‹•çš„ç¯„å›²
            dynamic_range = 20 * np.log10(np.max(np.abs(audio)) / (np.mean(np.abs(audio)) + 1e-10))
            
            # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            snr_score = min(1.0, max(0.0, (snr + 10) / 40))  # -10dB to 30dB
            voice_score = min(1.0, voice_ratio * 2)  # éŸ³å£°å‘¨æ³¢æ•°ç¯„å›²ã®æ¯”ç‡
            dynamic_score = min(1.0, max(0.0, dynamic_range / 40))  # å‹•çš„ç¯„å›²
            
            quality_score = (snr_score + voice_score + dynamic_score) / 3
            
            return quality_score
            
        except Exception as e:
            logger.warning(f"âš ï¸ å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5
    
    def extract_phonetic_features(self, audio: np.ndarray) -> Dict:
        """éŸ³éŸ»ç‰¹å¾´ã‚’æŠ½å‡º"""
        try:
            # 1. åŸºæœ¬éŸ³éŸ¿ç‰¹å¾´
            mfcc = librosa.feature.mfcc(y=audio, sr=self.target_sample_rate, n_mfcc=13)
            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=self.target_sample_rate)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.target_sample_rate)
            zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)
            
            # 2. ãƒ”ãƒƒãƒç‰¹å¾´
            pitches, magnitudes = librosa.piptrack(y=audio, sr=self.target_sample_rate)
            pitch_values = pitches[pitches > 0]
            
            # 3. ãƒªã‚ºãƒ ç‰¹å¾´
            tempo, beats = librosa.beat.beat_track(y=audio, sr=self.target_sample_rate)
            
            return {
                'mfcc_mean': np.mean(mfcc, axis=1).tolist(),
                'mfcc_std': np.std(mfcc, axis=1).tolist(),
                'spectral_centroid_mean': float(np.mean(spectral_centroid)),
                'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
                'zero_crossing_rate_mean': float(np.mean(zero_crossing_rate)),
                'pitch_mean': float(np.mean(pitch_values)) if len(pitch_values) > 0 else 0.0,
                'pitch_std': float(np.std(pitch_values)) if len(pitch_values) > 0 else 0.0,
                'tempo': float(tempo),
                'beat_count': len(beats)
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ éŸ³éŸ»ç‰¹å¾´æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def extract_dialect_features(self, audio: np.ndarray) -> Dict:
        """æ–¹è¨€ç‰¹å¾´ã‚’æŠ½å‡º"""
        try:
            # 1. éŸ³èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³
            pitches, magnitudes = librosa.piptrack(y=audio, sr=self.target_sample_rate)
            pitch_values = pitches[pitches > 0]
            
            # éŸ³èª¿ã®å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
            if len(pitch_values) > 1:
                pitch_changes = np.diff(pitch_values)
                pitch_variation = np.std(pitch_changes)
            else:
                pitch_variation = 0.0
            
            # 2. ãƒªã‚ºãƒ ç‰¹å¾´
            tempo, beats = librosa.beat.beat_track(y=audio, sr=self.target_sample_rate)
            
            # 3. éŸ³è‰²ç‰¹å¾´
            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=self.target_sample_rate)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.target_sample_rate)
            
            return {
                'pitch_variation': float(pitch_variation),
                'tempo': float(tempo),
                'spectral_centroid_mean': float(np.mean(spectral_centroid)),
                'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),
                'rhythm_regularity': float(np.std(beats)) if len(beats) > 1 else 0.0
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ–¹è¨€ç‰¹å¾´æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def extract_speaker_embedding(self, audio: np.ndarray) -> np.ndarray:
        """è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º"""
        try:
            # ç°¡æ˜“çš„ãªè©±è€…åŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
            mfcc = librosa.feature.mfcc(y=audio, sr=self.target_sample_rate, n_mfcc=13)
            
            # MFCCã®çµ±è¨ˆçš„ç‰¹å¾´ã‚’è©±è€…åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦ä½¿ç”¨
            embedding = np.concatenate([
                np.mean(mfcc, axis=1),  # å¹³å‡
                np.std(mfcc, axis=1),   # æ¨™æº–åå·®
                np.median(mfcc, axis=1) # ä¸­å¤®å€¤
            ])
            
            # æ­£è¦åŒ–
            embedding = embedding / (np.linalg.norm(embedding) + 1e-10)
            
            return embedding
            
        except Exception as e:
            logger.warning(f"âš ï¸ è©±è€…åŸ‹ã‚è¾¼ã¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros(39)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåŸ‹ã‚è¾¼ã¿
    
    def save_processed_audio(self, audio: np.ndarray, metadata: Dict) -> str:
        """å‰å‡¦ç†æ¸ˆã¿éŸ³å£°ã‚’ä¿å­˜"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            original_name = Path(metadata['file_path']).stem
            output_file = self.output_dir / f"processed_{original_name}.wav"
            
            # éŸ³å£°ã‚’ä¿å­˜
            sf.write(str(output_file), audio, self.target_sample_rate)
            
            return str(output_file)
            
        except Exception as e:
            logger.error(f"âŒ éŸ³å£°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def save_processed_metadata(self, processed_audio_list: List[ProcessedAudio]):
        """å‰å‡¦ç†æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            metadata_file = self.output_dir / "processed_metadata.json"
            
            metadata = []
            for audio in processed_audio_list:
                metadata.append({
                    'file_path': audio.file_path,
                    'original_path': audio.original_path,
                    'duration': audio.duration,
                    'sample_rate': audio.sample_rate,
                    'quality_score': audio.quality_score,
                    'phonetic_features': audio.phonetic_features,
                    'dialect_features': audio.dialect_features,
                    'speaker_embedding': audio.speaker_embedding.tolist()
                })
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {metadata_file}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸµ éŸ³å£°å‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")
    
    # å‰å‡¦ç†å™¨ã‚’åˆæœŸåŒ–
    preprocessor = AudioPreprocessor()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    metadata_file = "shanghai_audio_data/collected_metadata.json"
    
    if not os.path.exists(metadata_file):
        logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_file}")
        return
    
    # éŸ³å£°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰å‡¦ç†
    processed_audio_list = preprocessor.process_audio_collection(metadata_file)
    
    # å‰å‡¦ç†æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    preprocessor.save_processed_metadata(processed_audio_list)
    
    logger.info(f"ğŸ‰ å‰å‡¦ç†å®Œäº†: {len(processed_audio_list)}ä»¶")
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    if processed_audio_list:
        total_duration = sum(audio.duration for audio in processed_audio_list)
        avg_quality = sum(audio.quality_score for audio in processed_audio_list) / len(processed_audio_list)
        
        logger.info(f"ğŸ“Š çµ±è¨ˆæƒ…å ±:")
        logger.info(f"  - ç·æ™‚é–“: {total_duration/3600:.1f}æ™‚é–“")
        logger.info(f"  - å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.2f}")
        logger.info(f"  - è©±è€…æ•°: {len(set(audio.speaker_embedding.tobytes() for audio in processed_audio_list))}")

if __name__ == "__main__":
    main()