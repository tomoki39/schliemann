#!/usr/bin/env python3
"""
æ”¹è‰¯ç‰ˆAIéŸ³å£°å¤‰æ›TTSã‚·ã‚¹ãƒ†ãƒ 
Google Cloud TTS + éŸ³éŸ»å¤‰æ› + éŸ»å¾‹èª¿æ•´ã§è‡ªç„¶ãªä¸Šæµ·èªéŸ³å£°ã‚’ç”Ÿæˆ
"""

import os
import torch
import numpy as np
import soundfile as sf
import librosa
from pathlib import Path
import logging
from typing import Optional, Dict, Any, Tuple
import json
import asyncio
import tempfile
import subprocess
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedAITTSystem:
    """æ”¹è‰¯ç‰ˆAIéŸ³å£°å¤‰æ›TTSã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_dir: str = "models/enhanced_ai_tts"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sample_rate = 22050
        
        # ä¸Šæµ·èªèªå½™ãƒãƒƒãƒ—ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        self.vocabulary_map = self._load_enhanced_vocabulary_map()
        
        # éŸ³éŸ»å¤‰æ›ãƒ«ãƒ¼ãƒ«
        self.phonetic_rules = self._load_phonetic_rules()
        
        # éŸ»å¾‹èª¿æ•´ãƒ«ãƒ¼ãƒ«
        self.prosody_rules = self._load_prosody_rules()
        
    def _load_enhanced_vocabulary_map(self) -> Dict[str, str]:
        """æ‹¡å¼µç‰ˆä¸Šæµ·èªèªå½™ãƒãƒƒãƒ—ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            # åŸºæœ¬æŒ¨æ‹¶
            "ä½ å¥½": "ä¾¬å¥½", "æ‚¨å¥½": "ä¾¬å¥½", "å†è§": "å†ä¼š", "è°¢è°¢": "è°¢è°¢ä¾¬",
            "å¯¹ä¸èµ·": "å¯¹å‹¿èµ·", "æ²¡å…³ç³»": "å‘’æ²¡å…³ç³»", "ä¸å®¢æ°”": "å‹¿è¦å®¢æ°”",
            
            # æ™‚é–“ãƒ»æ—¥ä»˜
            "ä»Šå¤©": "ä»Šæœ", "æ˜å¤©": "æ˜æœ", "æ˜¨å¤©": "æ˜¨æ—¥", "ç°åœ¨": "ç°åœ¨",
            "æ—©ä¸Š": "æ—©æµª", "æ™šä¸Š": "å¤œåˆ°", "ä¸­åˆ": "ä¸­æµª", "ä¸‹åˆ": "ä¸‹åŠå¤©",
            
            # å®¶æ—ãƒ»äºº
            "æˆ‘ä»¬": "é˜¿æ‹‰", "ä½ ä»¬": "ä¾¬æ‹‰", "ä»–ä»¬": "ä¼Šæ‹‰", "æˆ‘": "é˜¿æ‹‰",
            "ä½ ": "ä¾¬", "ä»–": "ä¼Š", "å¥¹": "ä¼Š", "çˆ¸çˆ¸": "é˜¿çˆ¸", "å¦ˆå¦ˆ": "å§†å¦ˆ",
            "çˆ·çˆ·": "é˜¿çˆ·", "å¥¶å¥¶": "é˜¿å¥¶", "å“¥å“¥": "é˜¿å“¥", "å§å§": "é˜¿å§",
            
            # å ´æ‰€ãƒ»æ–¹å‘
            "å“ªé‡Œ": "å•¥åœ°æ–¹", "è¿™é‡Œ": "æ¿æ­", "é‚£é‡Œ": "ä¼Šæ­", "å®¶": "å±‹é‡Œ",
            "å­¦æ ¡": "å­¦å ‚", "åŒ»é™¢": "åŒ»é™¢", "å•†åº—": "å•†åº—", "é“¶è¡Œ": "é“¶è¡Œ",
            
            # å‹•ä½œãƒ»å‹•è©
            "åƒ": "åƒ", "å–": "åƒ", "èµ°": "è·‘", "è·‘": "è·‘", "å": "å",
            "ç«™": "ç«‹", "ç¡": "å›°è§‰", "å·¥ä½œ": "åšç”Ÿæ´»", "å­¦ä¹ ": "è¯»ä¹¦",
            "çœ‹": "çœ‹", "å¬": "å¬", "è¯´": "è®²", "æƒ³": "æƒ³",
            
            # å½¢å®¹è©ãƒ»çŠ¶æ…‹
            "å¥½": "å¥½", "å¾ˆå¥½": "è›®å¥½", "ä¸å¥½": "å‹¿å¥½", "å¤§": "å¤§", "å°": "å°",
            "å¤š": "å¤š", "å°‘": "å°‘", "å¿«": "å¿«", "æ…¢": "æ…¢", "çƒ­": "çƒ­", "å†·": "å†·",
            
            # ç–‘å•è©
            "ä»€ä¹ˆ": "å•¥", "æ€ä¹ˆ": "å“ªèƒ½", "ä¸ºä»€ä¹ˆ": "ä¸ºå•¥", "ä»€ä¹ˆæ—¶å€™": "å•¥è¾°å…‰",
            "å¤šå°‘": "å‡ åŒ–", "å‡ ä¸ª": "å‡ ä¸ª", "å“ªä¸ª": "å“ªä¸ª",
            
            # å¸¸ç”¨èª
            "æ˜¯": "æ˜¯", "ä¸æ˜¯": "å‹¿æ˜¯", "æœ‰": "æœ‰", "æ²¡æœ‰": "å‘’æ²¡",
            "å¯ä»¥": "å¯ä»¥", "ä¸å¯ä»¥": "å‹¿å¯ä»¥", "è¦": "è¦", "ä¸è¦": "å‹¿è¦",
            "ä¼š": "ä¼š", "ä¸ä¼š": "å‹¿ä¼š", "èƒ½": "èƒ½", "ä¸èƒ½": "å‹¿èƒ½",
            
            # æ„Ÿæƒ…ãƒ»æ„Ÿå˜†
            "å•Š": "å•Š", "å“¦": "å“¦", "å—¯": "å—¯", "å“å‘€": "å“å‘€",
            "çœŸçš„": "çœŸä¸ª", "å‡çš„": "å‡ä¸ª", "å½“ç„¶": "å½“ç„¶",
            
            # æ•°è©
            "ä¸€": "ä¸€", "äºŒ": "ä¸¤", "ä¸‰": "ä¸‰", "å››": "å››", "äº”": "äº”",
            "å…­": "å…­", "ä¸ƒ": "ä¸ƒ", "å…«": "å…«", "ä¹": "ä¹", "å": "å",
            
            # å¤©æ°—ãƒ»è‡ªç„¶
            "å¤©æ°”": "å¤©æ°”", "å¤ªé˜³": "å¤ªé˜³", "æœˆäº®": "æœˆäº®", "é›¨": "é›¨",
            "é›ª": "é›ª", "é£": "é£", "äº‘": "äº‘", "å¤©ç©º": "å¤©ç©º",
            
            # é£Ÿã¹ç‰©
            "é¥­": "é¥­", "èœ": "èœ", "è‚‰": "è‚‰", "é±¼": "é±¼", "é¸¡": "é¸¡",
            "è›‹": "è›‹", "é¢": "é¢", "æ±¤": "æ±¤", "èŒ¶": "èŒ¶", "æ°´": "æ°´",
            
            # è‰²
            "çº¢": "çº¢", "è“": "è“", "ç»¿": "ç»¿", "é»„": "é»„", "é»‘": "é»‘",
            "ç™½": "ç™½", "ç°": "ç°", "ç´«": "ç´«", "ç²‰": "ç²‰",
        }
    
    def _load_phonetic_rules(self) -> Dict[str, str]:
        """éŸ³éŸ»å¤‰æ›ãƒ«ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            # å­éŸ³å¤‰åŒ–
            "zh": "z", "ch": "c", "sh": "s", "r": "l",
            "j": "z", "q": "c", "x": "s",
            "n": "n", "l": "l", "h": "h", "f": "f",
            
            # æ¯éŸ³å¤‰åŒ–
            "an": "ang", "en": "eng", "in": "ing", "un": "ung",
            "ian": "iang", "uan": "uang", "Ã¼an": "Ã¼ang",
            "ei": "ai", "ui": "uei", "ou": "ou",
            
            # å£°èª¿å¤‰åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            "Ä": "a", "Ã¡": "a", "Ç": "a", "Ã ": "a",
            "Ä“": "e", "Ã©": "e", "Ä›": "e", "Ã¨": "e",
            "Ä«": "i", "Ã­": "i", "Ç": "i", "Ã¬": "i",
            "Å": "o", "Ã³": "o", "Ç’": "o", "Ã²": "o",
            "Å«": "u", "Ãº": "u", "Ç”": "u", "Ã¹": "u",
            "Ç–": "Ã¼", "Ç˜": "Ã¼", "Çš": "Ã¼", "Çœ": "Ã¼",
        }
    
    def _load_prosody_rules(self) -> Dict[str, Any]:
        """éŸ»å¾‹èª¿æ•´ãƒ«ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            "speaking_rate": 0.85,  # è©±é€Ÿã‚’å°‘ã—é…ã
            "pitch": 1.1,           # ãƒ”ãƒƒãƒã‚’å°‘ã—é«˜ã
            "volume_gain_db": 2.0,  # éŸ³é‡ã‚’å°‘ã—ä¸Šã’ã‚‹
            "pause_duration": 0.3,  # ãƒãƒ¼ã‚ºã®é•·ã•
            "emphasis_words": ["ä¾¬å¥½", "ä»Šæœ", "è›®å¥½", "é˜¿æ‹‰", "å•¥", "å“ªèƒ½"],
        }
    
    def convert_text_to_shanghai(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸Šæµ·èªã«å¤‰æ›"""
        converted_text = text
        
        # èªå½™å¤‰æ›
        for standard, shanghai in self.vocabulary_map.items():
            converted_text = converted_text.replace(standard, shanghai)
        
        # éŸ³éŸ»å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        for standard, shanghai in self.phonetic_rules.items():
            converted_text = converted_text.replace(standard, shanghai)
        
        return converted_text
    
    def generate_ssml_for_shanghai(self, text: str) -> str:
        """ä¸Šæµ·èªç”¨ã®SSMLã‚’ç”Ÿæˆ"""
        # å¼·èª¿ã™ã¹ãå˜èªã‚’ç‰¹å®š
        emphasis_words = self.prosody_rules["emphasis_words"]
        
        # æ–‡ã‚’åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        ssml_parts = []
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            sentence = sentence.strip()
            
            # å¼·èª¿å˜èªã«ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ã‚’è¿½åŠ 
            for word in emphasis_words:
                if word in sentence:
                    sentence = sentence.replace(word, f'<emphasis level="strong">{word}</emphasis>')
            
            # éŸ»å¾‹èª¿æ•´ã‚’é©ç”¨
            ssml_sentence = f'''
            <prosody rate="{self.prosody_rules['speaking_rate']}" 
                     pitch="+{int((self.prosody_rules['pitch'] - 1) * 100)}%" 
                     volume="+{int(self.prosody_rules['volume_gain_db'])}dB">
                {sentence}
            </prosody>
            '''
            ssml_parts.append(ssml_sentence)
        
        # SSMLã‚’çµ„ã¿ç«‹ã¦
        ssml = f'''
        <speak>
            {''.join(ssml_parts)}
        </speak>
        '''
        
        return ssml.strip()
    
    async def synthesize_dialect_audio(self, text: str, dialect: str = "shanghai") -> np.ndarray:
        """æ–¹è¨€éŸ³å£°ã‚’åˆæˆï¼ˆã™ã¹ã¦ã®æ–¹è¨€ã§åŒã˜å‡¦ç†ï¼‰"""
        logger.info(f"ğŸ—£ï¸ {dialect}æ–¹è¨€éŸ³å£°åˆæˆé–‹å§‹: {text}")
        
        # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ï¼ˆæ–¹è¨€ã«é–¢ä¿‚ãªãåŒã˜å‡¦ç†ï¼‰
        converted_text = text
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›: {text} â†’ {converted_text}")
        
        # åŸºæœ¬çš„ãªSSMLã‚’ç”Ÿæˆ
        ssml_text = f"<speak><voice name='zh-CN-Wavenet-A'>{converted_text}</voice></speak>"
        logger.info(f"SSMLç”Ÿæˆå®Œäº†")
        
        # Google Cloud TTSã‚’ä½¿ç”¨ã—ã¦éŸ³å£°åˆæˆ
        try:
            audio_data = await self._synthesize_with_google_cloud(ssml_text)
            logger.info(f"âœ… {dialect}æ–¹è¨€éŸ³å£°åˆæˆå®Œäº†")
            return audio_data
        except Exception as e:
            logger.error(f"âŒ Google Cloud TTS ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªéŸ³å£°ç”Ÿæˆ
            return await self._generate_fallback_audio(converted_text)
    
    async def _synthesize_with_google_cloud(self, ssml_text: str) -> np.ndarray:
        """Google Cloud TTSã‚’ä½¿ç”¨ã—ã¦éŸ³å£°åˆæˆï¼ˆPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªç‰ˆï¼‰"""
        try:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
            api_key = os.getenv('GOOGLE_CLOUD_API_KEY')
            if not api_key:
                logger.warning("GOOGLE_CLOUD_API_KEY not found, using fallback")
                raise ValueError("GOOGLE_CLOUD_API_KEY not found")
            
            # Google Cloud TTS Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
            from google.cloud import texttospeech
            
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            client = texttospeech.TextToSpeechClient()
            
            # éŸ³å£°è¨­å®š
            voice = texttospeech.VoiceSelectionParams(
                language_code="cmn-CN",
                name="cmn-CN-Wavenet-A",  # ä¸­å›½èªï¼ˆåŒ—äº¬èªï¼‰éŸ³å£°
                ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,
            )
            
            # éŸ³å£°è¨­å®š
            audio_config = texttospeech.AudioConfig(
                audio_encoding=texttospeech.AudioEncoding.LINEAR16,
                sample_rate_hertz=22050,
            )
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
            synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)
            
            # éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
            response = client.synthesize_speech(
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config
            )
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            audio_content = response.audio_content
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
                f.write(audio_content)
                audio_file = f.name
            
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            audio_data, sample_rate = librosa.load(audio_file, sr=self.sample_rate)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(audio_file):
                os.unlink(audio_file)
            
            logger.info(f"Google Cloud TTS éŸ³å£°ç”ŸæˆæˆåŠŸ: {len(audio_data)} samples, {len(audio_data)/sample_rate:.2f}s")
            return audio_data
            
        except ImportError:
            logger.warning("Google Cloud TTS ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            raise Exception("Google Cloud TTS library not installed")
        except Exception as e:
            logger.warning(f"Google Cloud TTS å¤±æ•—: {e}")
            raise
    
    async def _generate_fallback_audio(self, text: str) -> np.ndarray:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŸ³å£°ç”Ÿæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ç”Ÿæˆã‚’ä½¿ç”¨ï¼‰"""
        logger.info("ğŸ”„ ã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ç”Ÿæˆã‚’ä½¿ç”¨")
        
        try:
            from simple_voice_generator import simple_voice_generator
            audio = await simple_voice_generator.generate_voice(text)
            logger.info(f"ã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ç”ŸæˆæˆåŠŸ: {len(audio)} samples, {len(audio)/self.sample_rate:.2f}s")
            return audio
        except Exception as e:
            logger.warning(f"ã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ç”Ÿæˆå¤±æ•—: {e}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._generate_basic_fallback_audio(text)
    
    async def _generate_basic_fallback_audio(self, text: str) -> np.ndarray:
        """åŸºæœ¬çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŸ³å£°ç”Ÿæˆ"""
        logger.info("ğŸ”„ åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŸ³å£°ç”Ÿæˆã‚’ä½¿ç”¨")
        
        # ã‚ˆã‚Šè‡ªç„¶ãªéŸ³å£°ã‚’ç”Ÿæˆ
        duration = len(text) * 0.4  # æ–‡å­—æ•°ã«å¿œã˜ã¦é•·ã•ã‚’èª¿æ•´
        t = np.linspace(0, duration, int(self.sample_rate * duration))
        
        # è¤‡æ•°ã®å‘¨æ³¢æ•°ã§éŸ³å£°ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šè‡ªç„¶ãªéŸ³è‰²ï¼‰
        base_freq = 220  # A3
        frequencies = [base_freq, base_freq * 1.5, base_freq * 2, base_freq * 3]
        audio = np.zeros_like(t)
        
        # å„æ–‡å­—ã«å¯¾å¿œã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        char_duration = duration / len(text)
        
        for i, char in enumerate(text):
            if char.strip():  # ç©ºç™½æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                start_time = i * char_duration
                end_time = (i + 1) * char_duration
                
                # æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
                start_idx = int(start_time * self.sample_rate)
                end_idx = int(end_time * self.sample_rate)
                
                if start_idx < len(t) and end_idx <= len(t):
                    segment_t = t[start_idx:end_idx]
                    
                    # æ–‡å­—ã«å¿œã˜ã¦å‘¨æ³¢æ•°ã‚’é¸æŠ
                    freq = frequencies[i % len(frequencies)]
                    
                    # ã‚ˆã‚Šè‡ªç„¶ãªéŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ
                    fundamental = np.sin(2 * np.pi * freq * segment_t)
                    harmonic2 = 0.3 * np.sin(2 * np.pi * freq * 2 * segment_t)
                    harmonic3 = 0.1 * np.sin(2 * np.pi * freq * 3 * segment_t)
                    
                    segment_audio = fundamental + harmonic2 + harmonic3
                    
                    # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨ï¼ˆã‚ˆã‚Šè‡ªç„¶ãªæ¸›è¡°ï¼‰
                    envelope = np.exp(-segment_t * 1.5) * (1 + 0.1 * np.sin(2 * np.pi * 5 * segment_t))
                    segment_audio *= envelope
                    
                    # éŸ³é‡ã‚’èª¿æ•´
                    segment_audio *= 0.3
                    
                    audio[start_idx:end_idx] += segment_audio
        
        # æ­£è¦åŒ–
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.5
        
        return audio
    
    def get_audio_info(self, audio: np.ndarray) -> Dict[str, Any]:
        """éŸ³å£°æƒ…å ±ã‚’å–å¾—"""
        duration = len(audio) / self.sample_rate
        rms = np.sqrt(np.mean(audio**2))
        peak = np.max(np.abs(audio))
        
        # ãƒ‰ãƒŸãƒŠãƒ³ãƒˆå‘¨æ³¢æ•°ã‚’è¨ˆç®—
        fft = np.fft.fft(audio)
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        dominant_freq = freqs[np.argmax(np.abs(fft))]
        
        return {
            'duration': duration,
            'sample_rate': self.sample_rate,
            'rms': float(rms),
            'peak': float(peak),
            'dominant_frequency': float(abs(dominant_freq)),
            'length_samples': len(audio)
        }
    
    def get_vocabulary_count(self) -> int:
        """èªå½™æ•°ã‚’å–å¾—"""
        return len(self.vocabulary_map)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
enhanced_tts_system = EnhancedAITTSystem()

async def synthesize_dialect_audio(text: str, dialect: str = "shanghai") -> np.ndarray:
    """æ–¹è¨€éŸ³å£°ã‚’åˆæˆï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return await enhanced_tts_system.synthesize_dialect_audio(text, dialect)

def get_audio_info(audio: np.ndarray) -> Dict[str, Any]:
    """éŸ³å£°æƒ…å ±ã‚’å–å¾—ï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return enhanced_tts_system.get_audio_info(audio)

def get_vocabulary_count() -> int:
    """èªå½™æ•°ã‚’å–å¾—ï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return enhanced_tts_system.get_vocabulary_count()

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆ
    async def test():
        text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"
        audio = await synthesize_dialect_audio(text, "shanghai")
        info = get_audio_info(audio)
        print(f"éŸ³å£°æƒ…å ±: {info}")
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        sf.write("test_dialect.wav", audio, 22050)
        print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: test_dialect.wav")
    
    asyncio.run(test())
