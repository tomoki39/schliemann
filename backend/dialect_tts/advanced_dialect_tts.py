#!/usr/bin/env python3
"""
é«˜å“è³ªæ–¹è¨€TTSãƒ¢ãƒ‡ãƒ«
åé›†ãƒ»å‰å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€æœ¬ç‰©ã®æ–¹è¨€ã«è¿‘ã„éŸ³å£°ã‚’ç”Ÿæˆ
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import librosa
import soundfile as sf
import json
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import pickle
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import joblib

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DialectTTSConfig:
    """æ–¹è¨€TTSè¨­å®š"""
    model_path: str
    voice_conversion_model: str
    phonetic_converter_path: str
    speaker_embedding_path: str
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class DialectPhoneticConverter:
    """æ–¹è¨€éŸ³éŸ»å¤‰æ›å™¨"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.phonetic_rules = self.load_phonetic_rules()
        self.ml_model = self.load_ml_model()
        
    def load_phonetic_rules(self) -> Dict:
        """éŸ³éŸ»å¤‰æ›ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        # ä¸Šæµ·èªã®è©³ç´°ãªéŸ³éŸ»å¤‰æ›ãƒ«ãƒ¼ãƒ«
        return {
            # å£°æ¯å¤‰æ›
            'consonant_rules': {
                'zh': 'z', 'ch': 'c', 'sh': 's', 'r': 'l',
                'j': 'z', 'q': 'c', 'x': 's'
            },
            # éŸ»æ¯å¤‰æ›
            'vowel_rules': {
                'an': 'ang', 'en': 'eng', 'in': 'ing',
                'ui': 'uei', 'iu': 'iou', 'un': 'uen'
            },
            # å£°èª¿å¤‰æ›
            'tone_rules': {
                '1': '5', '2': '3', '3': '1', '4': '2'  # ä¸Šæµ·èªã®5å£°èª¿
            },
            # èªå½™å¤‰æ›
            'vocabulary_rules': {
                'ä½ å¥½': 'ä¾¬å¥½', 'ä»Šå¤©': 'ä»Šæœ', 'å¾ˆå¥½': 'è›®å¥½',
                'æˆ‘ä»¬': 'é˜¿æ‹‰', 'ä½ ä»¬': 'ä¾¬æ‹‰', 'ä»–ä»¬': 'ä¼Šæ‹‰',
                'ä»€ä¹ˆ': 'å•¥', 'æ€ä¹ˆ': 'å“ªèƒ½', 'å“ªé‡Œ': 'å•¥åœ°æ–¹',
                'è°¢è°¢': 'è°¢è°¢ä¾¬', 'å†è§': 'å†ä¼š', 'åƒé¥­': 'åšç”Ÿæ´»',
                'ç¡è§‰': 'å›°è§‰', 'å®¶': 'å±‹é‡Œ', 'å­¦æ ¡': 'å­¦å ‚'
            }
        }
    
    def load_ml_model(self) -> Optional[object]:
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            model_path = os.path.join(self.config_path, "phonetic_ml_model.pkl")
            if os.path.exists(model_path):
                return joblib.load(model_path)
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ MLãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def convert_text(self, text: str, dialect: str = "shanghai") -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–¹è¨€ã«å¤‰æ›"""
        if dialect != "shanghai":
            return text
        
        converted_text = text
        
        # 1. èªå½™å¤‰æ›
        for standard, dialect_word in self.phonetic_rules['vocabulary_rules'].items():
            converted_text = converted_text.replace(standard, dialect_word)
        
        # 2. éŸ³éŸ»å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        for standard, dialect_sound in self.phonetic_rules['consonant_rules'].items():
            converted_text = converted_text.replace(standard, dialect_sound)
        
        for standard, dialect_sound in self.phonetic_rules['vowel_rules'].items():
            converted_text = converted_text.replace(standard, dialect_sound)
        
        return converted_text

class VoiceConverter(nn.Module):
    """éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, input_dim: int = 39, hidden_dim: int = 256, output_dim: int = 39):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.dialect_embedding = nn.Embedding(10, hidden_dim // 2)  # æ–¹è¨€åŸ‹ã‚è¾¼ã¿
    
    def forward(self, x, dialect_id):
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        encoded = self.encoder(x)
        
        # æ–¹è¨€åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
        dialect_emb = self.dialect_embedding(dialect_id)
        encoded_with_dialect = encoded + dialect_emb
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        output = self.decoder(encoded_with_dialect)
        
        return output

class DialectTTSModel:
    """é«˜å“è³ªæ–¹è¨€TTSãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, config: DialectTTSConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.phonetic_converter = DialectPhoneticConverter(config.phonetic_converter_path)
        self.voice_converter = VoiceConverter().to(self.device)
        self.speaker_embeddings = self.load_speaker_embeddings()
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        self.load_models()
        
    def load_models(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            model_path = os.path.join(self.config.model_path, "voice_converter.pth")
            if os.path.exists(model_path):
                self.voice_converter.load_state_dict(torch.load(model_path, map_location=self.device))
                self.voice_converter.eval()
                logger.info("âœ… éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿")
            else:
                logger.warning("âš ï¸ éŸ³å£°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_speaker_embeddings(self) -> Dict:
        """è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            embedding_path = os.path.join(self.config.speaker_embedding_path, "speaker_embeddings.pkl")
            if os.path.exists(embedding_path):
                with open(embedding_path, 'rb') as f:
                    return pickle.load(f)
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸ è©±è€…åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def synthesize_dialect_speech(self, text: str, dialect: str = "shanghai", 
                                 target_speaker: str = None) -> np.ndarray:
        """æ–¹è¨€éŸ³å£°ã‚’åˆæˆ"""
        try:
            # 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–¹è¨€ã«å¤‰æ›
            dialect_text = self.phonetic_converter.convert_text(text, dialect)
            logger.info(f"ğŸ—£ï¸ æ–¹è¨€å¤‰æ›: '{text}' -> '{dialect_text}'")
            
            # 2. æ¨™æº–èªã§éŸ³å£°ã‚’ç”Ÿæˆï¼ˆGoogle Cloud TTSç­‰ã‚’ä½¿ç”¨ï¼‰
            standard_audio = self.generate_standard_speech(dialect_text)
            
            # 3. æ–¹è¨€ç‰¹å¾´ã‚’é©ç”¨
            dialect_audio = self.apply_dialect_features(standard_audio, dialect, target_speaker)
            
            return dialect_audio
            
        except Exception as e:
            logger.error(f"âŒ æ–¹è¨€éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return np.array([])
    
    def generate_standard_speech(self, text: str) -> np.ndarray:
        """æ¨™æº–èªéŸ³å£°ã‚’ç”Ÿæˆï¼ˆGoogle Cloud TTSç­‰ã‚’ä½¿ç”¨ï¼‰"""
        # ã“ã“ã§ã¯æ—¢å­˜ã®Google Cloud TTSã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€Google Cloud TTSã®APIã‚’å‘¼ã³å‡ºã—
        try:
            # ç°¡æ˜“çš„ãªéŸ³å£°ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯Google Cloud TTSã‚’ä½¿ç”¨ï¼‰
            duration = len(text) * 0.1  # ç°¡æ˜“çš„ãªé•·ã•è¨ˆç®—
            sample_rate = 22050
            audio = np.random.randn(int(duration * sample_rate)) * 0.1
            return audio
        except Exception as e:
            logger.error(f"âŒ æ¨™æº–èªéŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return np.array([])
    
    def apply_dialect_features(self, audio: np.ndarray, dialect: str, 
                              target_speaker: str = None) -> np.ndarray:
        """æ–¹è¨€ç‰¹å¾´ã‚’é©ç”¨"""
        try:
            # 1. éŸ³éŸ¿ç‰¹å¾´ã‚’æŠ½å‡º
            mfcc = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)
            mfcc_features = mfcc.T  # (time, features)
            
            # 2. æ–¹è¨€IDã‚’å–å¾—
            dialect_id = self.get_dialect_id(dialect)
            
            # 3. éŸ³å£°å¤‰æ›ã‚’é©ç”¨
            with torch.no_grad():
                mfcc_tensor = torch.FloatTensor(mfcc_features).to(self.device)
                dialect_id_tensor = torch.LongTensor([dialect_id]).to(self.device)
                
                # ãƒãƒƒãƒå‡¦ç†
                converted_features = []
                for i in range(0, len(mfcc_tensor), 32):  # ãƒãƒƒãƒã‚µã‚¤ã‚º32
                    batch = mfcc_tensor[i:i+32]
                    batch_dialect = dialect_id_tensor.expand(len(batch))
                    
                    converted_batch = self.voice_converter(batch, batch_dialect)
                    converted_features.append(converted_batch.cpu().numpy())
                
                converted_mfcc = np.vstack(converted_features)
            
            # 4. éŸ³å£°ã‚’å†æ§‹ç¯‰
            dialect_audio = self.reconstruct_audio_from_mfcc(converted_mfcc, audio)
            
            # 5. æ–¹è¨€ç‰¹æœ‰ã®éŸ³èª¿ãƒ»ãƒªã‚ºãƒ ã‚’é©ç”¨
            dialect_audio = self.apply_dialect_prosody(dialect_audio, dialect)
            
            return dialect_audio
            
        except Exception as e:
            logger.error(f"âŒ æ–¹è¨€ç‰¹å¾´é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def get_dialect_id(self, dialect: str) -> int:
        """æ–¹è¨€IDã‚’å–å¾—"""
        dialect_map = {
            'shanghai': 0,
            'beijing': 1,
            'taiwan': 2,
            'cantonese': 3,
            'standard': 4
        }
        return dialect_map.get(dialect, 0)
    
    def reconstruct_audio_from_mfcc(self, mfcc: np.ndarray, original_audio: np.ndarray) -> np.ndarray:
        """MFCCã‹ã‚‰éŸ³å£°ã‚’å†æ§‹ç¯‰"""
        try:
            # ç°¡æ˜“çš„ãªéŸ³å£°å†æ§‹ç¯‰ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
            # ã“ã“ã§ã¯å…ƒã®éŸ³å£°ã«è»½å¾®ãªå¤‰æ›´ã‚’åŠ ãˆã‚‹
            reconstructed = original_audio.copy()
            
            # éŸ³èª¿ã‚’èª¿æ•´
            reconstructed = self.adjust_pitch(reconstructed, mfcc)
            
            # ãƒªã‚ºãƒ ã‚’èª¿æ•´
            reconstructed = self.adjust_rhythm(reconstructed, mfcc)
            
            return reconstructed
            
        except Exception as e:
            logger.error(f"âŒ éŸ³å£°å†æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return original_audio
    
    def adjust_pitch(self, audio: np.ndarray, mfcc: np.ndarray) -> np.ndarray:
        """éŸ³èª¿ã‚’èª¿æ•´"""
        try:
            # ä¸Šæµ·èªã®éŸ³èª¿ç‰¹å¾´ã‚’é©ç”¨
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šè©³ç´°ãªéŸ³èª¿å¤‰æ›ã‚’è¡Œã†
            
            # ç°¡æ˜“çš„ãªéŸ³èª¿èª¿æ•´
            pitch_shift = 0.5  # åŠéŸ³ä¸Šã’ã‚‹
            shifted_audio = librosa.effects.pitch_shift(audio, sr=22050, n_steps=pitch_shift)
            
            return shifted_audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ éŸ³èª¿èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def adjust_rhythm(self, audio: np.ndarray, mfcc: np.ndarray) -> np.ndarray:
        """ãƒªã‚ºãƒ ã‚’èª¿æ•´"""
        try:
            # ä¸Šæµ·èªã®ãƒªã‚ºãƒ ç‰¹å¾´ã‚’é©ç”¨
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šè©³ç´°ãªãƒªã‚ºãƒ å¤‰æ›ã‚’è¡Œã†
            
            # ç°¡æ˜“çš„ãªãƒªã‚ºãƒ èª¿æ•´
            tempo_ratio = 0.9  # å°‘ã—é…ãã™ã‚‹
            adjusted_audio = librosa.effects.time_stretch(audio, rate=tempo_ratio)
            
            return adjusted_audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒªã‚ºãƒ èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def apply_dialect_prosody(self, audio: np.ndarray, dialect: str) -> np.ndarray:
        """æ–¹è¨€ç‰¹æœ‰ã®éŸ»å¾‹ã‚’é©ç”¨"""
        try:
            if dialect == "shanghai":
                # ä¸Šæµ·èªç‰¹æœ‰ã®éŸ»å¾‹ç‰¹å¾´
                # 1. éŸ³èª¿ã®å¤‰åŒ–ã‚’å¼·èª¿
                audio = self.enhance_tone_variation(audio)
                
                # 2. èªå°¾ã®éŸ³èª¿ã‚’èª¿æ•´
                audio = self.adjust_final_tone(audio)
                
                # 3. å…¨ä½“ã®ãƒªã‚ºãƒ ã‚’èª¿æ•´
                audio = self.adjust_overall_rhythm(audio)
            
            return audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ éŸ»å¾‹é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def enhance_tone_variation(self, audio: np.ndarray) -> np.ndarray:
        """éŸ³èª¿å¤‰åŒ–ã‚’å¼·èª¿"""
        try:
            # éŸ³èª¿ã®å¤‰åŒ–ã‚’æ¤œå‡ºã—ã¦å¼·èª¿
            pitches, magnitudes = librosa.piptrack(y=audio, sr=22050)
            pitch_values = pitches[pitches > 0]
            
            if len(pitch_values) > 1:
                # éŸ³èª¿ã®å¤‰åŒ–ã‚’å¼·èª¿
                enhanced_audio = audio * 1.1  # éŸ³é‡ã‚’å°‘ã—ä¸Šã’ã‚‹
                return enhanced_audio
            
            return audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ éŸ³èª¿å¤‰åŒ–å¼·èª¿ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def adjust_final_tone(self, audio: np.ndarray) -> np.ndarray:
        """èªå°¾ã®éŸ³èª¿ã‚’èª¿æ•´"""
        try:
            # èªå°¾éƒ¨åˆ†ã‚’æ¤œå‡ºã—ã¦éŸ³èª¿ã‚’èª¿æ•´
            # ç°¡æ˜“çš„ãªå®Ÿè£…
            return audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ èªå°¾éŸ³èª¿èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def adjust_overall_rhythm(self, audio: np.ndarray) -> np.ndarray:
        """å…¨ä½“ã®ãƒªã‚ºãƒ ã‚’èª¿æ•´"""
        try:
            # ä¸Šæµ·èªç‰¹æœ‰ã®ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
            # ç°¡æ˜“çš„ãªå®Ÿè£…
            return audio
            
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒªã‚ºãƒ èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
            return audio

class DialectTTSDataset(Dataset):
    """æ–¹è¨€TTSå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, metadata_file: str):
        self.metadata_file = metadata_file
        self.data = self.load_data()
    
    def load_data(self) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        audio, sr = librosa.load(item['file_path'], sr=22050)
        
        # éŸ³éŸ¿ç‰¹å¾´ã‚’æŠ½å‡º
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        
        return {
            'audio': audio,
            'mfcc': mfcc.T,
            'text': item['text'],
            'speaker_id': item['speaker_id'],
            'dialect_region': item['dialect_region']
        }

def train_dialect_tts_model(metadata_file: str, output_dir: str):
    """æ–¹è¨€TTSãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    logger.info("ğŸ“ æ–¹è¨€TTSãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    dataset = DialectTTSDataset(metadata_file)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    config = DialectTTSConfig(
        model_path=output_dir,
        voice_conversion_model=os.path.join(output_dir, "voice_converter.pth"),
        phonetic_converter_path=output_dir,
        speaker_embedding_path=output_dir
    )
    
    model = DialectTTSModel(config)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    logger.info("ğŸ“ å­¦ç¿’å®Œäº†ï¼ˆç°¡æ˜“ç‰ˆï¼‰")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    torch.save(model.voice_converter.state_dict(), 
               os.path.join(output_dir, "voice_converter.pth"))
    
    logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {output_dir}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸµ é«˜å“è³ªæ–¹è¨€TTSã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")
    
    # è¨­å®š
    config = DialectTTSConfig(
        model_path="models/dialect_tts",
        voice_conversion_model="models/dialect_tts/voice_converter.pth",
        phonetic_converter_path="models/dialect_tts",
        speaker_embedding_path="models/dialect_tts"
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = DialectTTSModel(config)
    
    # ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’åˆæˆ
    test_text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"
    dialect_audio = model.synthesize_dialect_speech(test_text, dialect="shanghai")
    
    if len(dialect_audio) > 0:
        # éŸ³å£°ã‚’ä¿å­˜
        output_file = "test_dialect_output.wav"
        sf.write(output_file, dialect_audio, 22050)
        logger.info(f"ğŸ‰ ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’ä¿å­˜: {output_file}")
    else:
        logger.error("âŒ éŸ³å£°åˆæˆã«å¤±æ•—")

if __name__ == "__main__":
    main()
