#!/usr/bin/env python3
"""
ãƒªã‚¢ãƒ«ãªéŸ³å£°ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
ã‚ˆã‚Šè‡ªç„¶ã§ç†è§£å¯èƒ½ãªéŸ³å£°ã‚’ç”Ÿæˆ
"""

import os
import numpy as np
import soundfile as sf
import librosa
import logging
from typing import Dict, Any
import asyncio
import subprocess
import tempfile
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealisticVoiceGenerator:
    """ãƒªã‚¢ãƒ«ãªéŸ³å£°ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.sample_rate = 22050
        
        # ä¸Šæµ·èªèªå½™ãƒãƒƒãƒ—
        self.vocabulary_map = {
            "ä½ å¥½": "ä¾¬å¥½", "ä»Šå¤©": "ä»Šæœ", "å¾ˆå¥½": "è›®å¥½", "æˆ‘ä»¬": "é˜¿æ‹‰",
            "ä»€ä¹ˆ": "å•¥", "æ€ä¹ˆ": "å“ªèƒ½", "å“ªé‡Œ": "å•¥åœ°æ–¹", "è°¢è°¢": "è°¢è°¢ä¾¬",
            "å†è§": "å†ä¼š", "å¯¹ä¸èµ·": "å¯¹å‹¿èµ·", "æ²¡å…³ç³»": "å‘’æ²¡å…³ç³»",
            "å®¶": "å±‹é‡Œ", "å­¦æ ¡": "å­¦å ‚", "å·¥ä½œ": "åšç”Ÿæ´»", "å­¦ä¹ ": "è¯»ä¹¦",
            "åƒ": "åƒ", "å–": "åƒ", "èµ°": "è·‘", "å": "å", "ç«™": "ç«‹",
            "ç¡": "å›°è§‰", "çœ‹": "çœ‹", "å¬": "å¬", "è¯´": "è®²", "æƒ³": "æƒ³",
            "æ˜¯": "æ˜¯", "ä¸æ˜¯": "å‹¿æ˜¯", "æœ‰": "æœ‰", "æ²¡æœ‰": "å‘’æ²¡",
            "å¯ä»¥": "å¯ä»¥", "ä¸å¯ä»¥": "å‹¿å¯ä»¥", "è¦": "è¦", "ä¸è¦": "å‹¿è¦",
            "ä¼š": "ä¼š", "ä¸ä¼š": "å‹¿ä¼š", "èƒ½": "èƒ½", "ä¸èƒ½": "å‹¿èƒ½",
            "ä¸€": "ä¸€", "äºŒ": "ä¸¤", "ä¸‰": "ä¸‰", "å››": "å››", "äº”": "äº”",
            "å…­": "å…­", "ä¸ƒ": "ä¸ƒ", "å…«": "å…«", "ä¹": "ä¹", "å": "å",
        }
    
    def convert_text_to_shanghai(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸Šæµ·èªã«å¤‰æ›"""
        converted_text = text
        
        # èªå½™å¤‰æ›
        for standard, shanghai in self.vocabulary_map.items():
            converted_text = converted_text.replace(standard, shanghai)
        
        return converted_text
    
    async def generate_voice(self, text: str) -> np.ndarray:
        """éŸ³å£°ã‚’ç”Ÿæˆ"""
        logger.info(f"ğŸ—£ï¸ ãƒªã‚¢ãƒ«éŸ³å£°ç”Ÿæˆé–‹å§‹: {text}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸Šæµ·èªã«å¤‰æ›
        shanghai_text = self.convert_text_to_shanghai(text)
        logger.info(f"èªå½™å¤‰æ›: {text} â†’ {shanghai_text}")
        
        # è¤‡æ•°ã®æ–¹æ³•ã§éŸ³å£°ç”Ÿæˆã‚’è©¦è¡Œ
        methods = [
            self._generate_with_tts_engines,
            self._generate_with_phonetic_synthesis,
            self._generate_with_melodic_synthesis,
            self._generate_with_basic_synthesis,
        ]
        
        for method in methods:
            try:
                audio = await method(shanghai_text)
                if audio is not None and len(audio) > 0 and len(audio) > 1000:  # æœ€ä½1ç§’ä»¥ä¸Š
                    logger.info(f"âœ… éŸ³å£°ç”ŸæˆæˆåŠŸ: {method.__name__} ({len(audio)} samples, {len(audio)/self.sample_rate:.2f}s)")
                    return audio
            except Exception as e:
                logger.warning(f"âš ï¸ {method.__name__} å¤±æ•—: {e}")
                continue
        
        # ã™ã¹ã¦ã®æ–¹æ³•ãŒå¤±æ•—ã—ãŸå ´åˆã€åŸºæœ¬çš„ãªéŸ³å£°ã‚’ç”Ÿæˆ
        logger.warning("âš ï¸ ã™ã¹ã¦ã®éŸ³å£°ç”Ÿæˆæ–¹æ³•ãŒå¤±æ•—ã€åŸºæœ¬éŸ³å£°ã‚’ç”Ÿæˆ")
        return await self._generate_basic_audio(shanghai_text)
    
    async def _generate_with_tts_engines(self, text: str) -> np.ndarray:
        """TTSã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # 1. macOSã®sayã‚³ãƒãƒ³ãƒ‰ï¼ˆè¤‡æ•°ã®éŸ³å£°ã§è©¦è¡Œï¼‰
            voices_to_try = ['Alex', 'Samantha', 'Victoria', 'Daniel', 'Karen']
            
            for voice in voices_to_try:
                try:
                    with tempfile.NamedTemporaryFile(suffix='.aiff', delete=False) as f:
                        temp_file = f.name
                    
                    # è‹±èªéŸ³å£°ã§è©¦è¡Œï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
                    # ä¸­å›½èªæ–‡å­—ã‚’è‹±èªã«å¤‰æ›ã—ã¦è©¦è¡Œ
                    english_text = "Hello, how are you today? This is a test of the voice synthesis system."
                    cmd = ['say', '-v', voice, '-r', '120', '-o', temp_file, english_text]
                    logger.info(f"sayã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
                    
                    if result.returncode == 0:
                        if os.path.exists(temp_file) and os.path.getsize(temp_file) > 0:
                            try:
                                audio, sr = librosa.load(temp_file, sr=self.sample_rate)
                                os.unlink(temp_file)
                                
                                if len(audio) > 1000:  # æœ€ä½1ç§’ä»¥ä¸Š
                                    logger.info(f"sayéŸ³å£°ç”ŸæˆæˆåŠŸ ({voice}): {len(audio)} samples, {len(audio)/self.sample_rate:.2f}s")
                                    return audio
                            except Exception as load_error:
                                logger.warning(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({voice}): {load_error}")
                        else:
                            logger.warning(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹ç©ºã§ã™ ({voice}): {temp_file}")
                    
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                        
                except Exception as e:
                    logger.warning(f"sayéŸ³å£° {voice} ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # 2. ä¸­å›½èªéŸ³å£°ã§è©¦è¡Œ
            chinese_voices = ['Ting-Ting', 'Sin-ji', 'Mei-Jia', 'Yu-shu']
            for voice in chinese_voices:
                try:
                    with tempfile.NamedTemporaryFile(suffix='.aiff', delete=False) as f:
                        temp_file = f.name
                    
                    cmd = ['say', '-v', voice, '-r', '100', '-o', temp_file, text]
                    logger.info(f"ä¸­å›½èªsayã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
                    
                    if result.returncode == 0:
                        if os.path.exists(temp_file) and os.path.getsize(temp_file) > 0:
                            try:
                                audio, sr = librosa.load(temp_file, sr=self.sample_rate)
                                os.unlink(temp_file)
                                
                                if len(audio) > 1000:  # æœ€ä½1ç§’ä»¥ä¸Š
                                    logger.info(f"ä¸­å›½èªsayéŸ³å£°ç”ŸæˆæˆåŠŸ ({voice}): {len(audio)} samples, {len(audio)/self.sample_rate:.2f}s")
                                    return audio
                            except Exception as load_error:
                                logger.warning(f"ä¸­å›½èªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({voice}): {load_error}")
                        else:
                            logger.warning(f"ä¸­å›½èªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹ç©ºã§ã™ ({voice}): {temp_file}")
                    
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                        
                except Exception as e:
                    logger.warning(f"ä¸­å›½èªsayéŸ³å£° {voice} ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            logger.warning(f"TTSã‚¨ãƒ³ã‚¸ãƒ³ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    async def _generate_with_phonetic_synthesis(self, text: str) -> np.ndarray:
        """éŸ³éŸ»åˆæˆã‚’ä½¿ç”¨ï¼ˆæ”¹è‰¯ç‰ˆ - ã‚ˆã‚Šè‡ªç„¶ãªéŸ³å£°ï¼‰"""
        try:
            # æ–‡å­—ã‚’éŸ³éŸ»ã«åˆ†è§£ã—ã¦åˆæˆ
            duration = max(3.0, len(text) * 0.8)  # æœ€ä½3ç§’ã€æ–‡å­—æ•°Ã—0.8ç§’
            t = np.linspace(0, duration, int(self.sample_rate * duration))
            
            # ã‚ˆã‚Šè‡ªç„¶ãªå‘¨æ³¢æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³
            base_freq = 150  # ã‚ˆã‚Šä½ã„åŸºæœ¬å‘¨æ³¢æ•°
            frequencies = [base_freq, base_freq * 1.2, base_freq * 1.4, base_freq * 1.6, base_freq * 1.8]
            audio = np.zeros_like(t)
            
            # å„æ–‡å­—ã«å¯¾å¿œã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            char_duration = duration / len(text)
            
            for i, char in enumerate(text):
                if char.strip():  # ç©ºç™½æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    start_time = i * char_duration
                    end_time = (i + 1) * char_duration
                    
                    # æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
                    start_idx = int(start_time * self.sample_rate)
                    end_idx = int(end_time * self.sample_rate)
                    
                    if start_idx < len(t) and end_idx <= len(t):
                        segment_t = t[start_idx:end_idx]
                        
                        # æ–‡å­—ã«å¿œã˜ã¦å‘¨æ³¢æ•°ã‚’é¸æŠï¼ˆã‚ˆã‚Šè‡ªç„¶ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                        freq = frequencies[i % len(frequencies)]
                        
                        # ã‚ˆã‚Šè‡ªç„¶ãªéŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆåˆæˆï¼‰
                        fundamental = np.sin(2 * np.pi * freq * segment_t)
                        harmonic2 = 0.3 * np.sin(2 * np.pi * freq * 2 * segment_t)
                        harmonic3 = 0.15 * np.sin(2 * np.pi * freq * 3 * segment_t)
                        harmonic4 = 0.08 * np.sin(2 * np.pi * freq * 4 * segment_t)
                        harmonic5 = 0.04 * np.sin(2 * np.pi * freq * 5 * segment_t)
                        
                        # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆåŠ¹æœã‚’è¿½åŠ 
                        formant1 = 0.1 * np.sin(2 * np.pi * (freq + 200) * segment_t)
                        formant2 = 0.05 * np.sin(2 * np.pi * (freq + 400) * segment_t)
                        
                        segment_audio = (fundamental + harmonic2 + harmonic3 + harmonic4 + harmonic5 + 
                                       formant1 + formant2)
                        
                        # ã‚ˆã‚Šè‡ªç„¶ãªã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨ï¼ˆADSRï¼‰
                        attack_time = 0.1
                        decay_time = 0.2
                        sustain_level = 0.7
                        release_time = 0.3
                        
                        envelope = np.ones_like(segment_t)
                        
                        # Attack
                        attack_samples = int(attack_time * self.sample_rate)
                        if attack_samples > 0:
                            envelope[:attack_samples] = np.linspace(0, 1, attack_samples)
                        
                        # Decay
                        decay_samples = int(decay_time * self.sample_rate)
                        if decay_samples > 0 and attack_samples + decay_samples < len(envelope):
                            envelope[attack_samples:attack_samples + decay_samples] = np.linspace(1, sustain_level, decay_samples)
                        
                        # Sustain
                        sustain_start = attack_samples + decay_samples
                        sustain_end = len(envelope) - int(release_time * self.sample_rate)
                        if sustain_end > sustain_start:
                            envelope[sustain_start:sustain_end] = sustain_level
                        
                        # Release
                        release_samples = int(release_time * self.sample_rate)
                        if release_samples > 0:
                            envelope[-release_samples:] = np.linspace(sustain_level, 0, release_samples)
                        
                        # å¾®ç´°ãªå¤‰èª¿ã‚’è¿½åŠ 
                        vibrato = 1 + 0.05 * np.sin(2 * np.pi * 5 * segment_t)
                        tremolo = 1 + 0.1 * np.sin(2 * np.pi * 3 * segment_t)
                        
                        segment_audio *= envelope * vibrato * tremolo
                        
                        # éŸ³é‡ã‚’èª¿æ•´
                        segment_audio *= 0.3
                        
                        audio[start_idx:end_idx] += segment_audio
            
            # æ­£è¦åŒ–
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.5
            
            return audio
            
        except Exception as e:
            logger.error(f"éŸ³éŸ»åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _generate_with_melodic_synthesis(self, text: str) -> np.ndarray:
        """ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯åˆæˆã‚’ä½¿ç”¨"""
        try:
            # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯ãªéŸ³å£°ã‚’ç”Ÿæˆ
            duration = max(3.0, len(text) * 0.8)  # æœ€ä½3ç§’
            t = np.linspace(0, duration, int(self.sample_rate * duration))
            
            # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯ãªå‘¨æ³¢æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³
            base_freq = 220  # A3
            melody_pattern = [1.0, 1.2, 1.0, 0.8, 1.5, 1.0, 0.9, 1.1]  # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³
            audio = np.zeros_like(t)
            
            # å„æ–‡å­—ã«å¯¾å¿œã™ã‚‹ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            char_duration = duration / len(text)
            
            for i, char in enumerate(text):
                if char.strip():
                    start_time = i * char_duration
                    end_time = (i + 1) * char_duration
                    
                    start_idx = int(start_time * self.sample_rate)
                    end_idx = int(end_time * self.sample_rate)
                    
                    if start_idx < len(t) and end_idx <= len(t):
                        segment_t = t[start_idx:end_idx]
                        
                        # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ã¦å‘¨æ³¢æ•°ã‚’é¸æŠ
                        freq_multiplier = melody_pattern[i % len(melody_pattern)]
                        freq = base_freq * freq_multiplier
                        
                        # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯ãªéŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ
                        fundamental = np.sin(2 * np.pi * freq * segment_t)
                        harmonic2 = 0.3 * np.sin(2 * np.pi * freq * 2 * segment_t)
                        harmonic3 = 0.15 * np.sin(2 * np.pi * freq * 3 * segment_t)
                        
                        segment_audio = fundamental + harmonic2 + harmonic3
                        
                        # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯ãªã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—
                        envelope = np.exp(-segment_t * 0.5) * (1 + 0.3 * np.sin(2 * np.pi * 3 * segment_t))
                        segment_audio *= envelope
                        
                        # éŸ³é‡ã‚’èª¿æ•´
                        segment_audio *= 0.3
                        
                        audio[start_idx:end_idx] += segment_audio
            
            # æ­£è¦åŒ–
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.5
            
            return audio
            
        except Exception as e:
            logger.error(f"ãƒ¡ãƒ­ãƒ‡ã‚£ãƒƒã‚¯åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _generate_with_basic_synthesis(self, text: str) -> np.ndarray:
        """åŸºæœ¬çš„ãªéŸ³å£°åˆæˆ"""
        try:
            # æ–‡å­—æ•°ã«å¿œã˜ã¦éŸ³å£°ã®é•·ã•ã‚’æ±ºå®š
            duration = max(2.0, len(text) * 0.5)  # æœ€ä½2ç§’ã€æ–‡å­—æ•°Ã—0.5ç§’
            t = np.linspace(0, duration, int(self.sample_rate * duration))
            
            # è¤‡æ•°ã®å‘¨æ³¢æ•°ã§éŸ³å£°ã‚’ç”Ÿæˆ
            base_freq = 220  # A3
            frequencies = [base_freq, base_freq * 1.5, base_freq * 2, base_freq * 3]
            audio = np.zeros_like(t)
            
            # å„æ–‡å­—ã«å¯¾å¿œã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            char_duration = duration / len(text)
            
            for i, char in enumerate(text):
                if char.strip():  # ç©ºç™½æ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    start_time = i * char_duration
                    end_time = (i + 1) * char_duration
                    
                    # æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
                    start_idx = int(start_time * self.sample_rate)
                    end_idx = int(end_time * self.sample_rate)
                    
                    if start_idx < len(t) and end_idx <= len(t):
                        segment_t = t[start_idx:end_idx]
                        
                        # æ–‡å­—ã«å¿œã˜ã¦å‘¨æ³¢æ•°ã‚’é¸æŠ
                        freq = frequencies[i % len(frequencies)]
                        
                        # éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ
                        fundamental = np.sin(2 * np.pi * freq * segment_t)
                        harmonic2 = 0.3 * np.sin(2 * np.pi * freq * 2 * segment_t)
                        harmonic3 = 0.1 * np.sin(2 * np.pi * freq * 3 * segment_t)
                        
                        segment_audio = fundamental + harmonic2 + harmonic3
                        
                        # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨
                        envelope = np.exp(-segment_t * 1.0) * (1 + 0.1 * np.sin(2 * np.pi * 3 * segment_t))
                        segment_audio *= envelope
                        
                        # éŸ³é‡ã‚’èª¿æ•´
                        segment_audio *= 0.2
                        
                        audio[start_idx:end_idx] += segment_audio
            
            # æ­£è¦åŒ–
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.5
            
            return audio
            
        except Exception as e:
            logger.error(f"åŸºæœ¬éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _generate_basic_audio(self, text: str) -> np.ndarray:
        """åŸºæœ¬çš„ãªéŸ³å£°ç”Ÿæˆï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        logger.info("ğŸ”„ åŸºæœ¬éŸ³å£°ç”Ÿæˆã‚’ä½¿ç”¨")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªéŸ³å£°ã‚’ç”Ÿæˆ
        duration = max(3.0, len(text) * 0.4)  # æœ€ä½3ç§’
        t = np.linspace(0, duration, int(self.sample_rate * duration))
        
        # åŸºæœ¬çš„ãªã‚µã‚¤ãƒ³æ³¢
        freq = 220  # A3
        audio = 0.3 * np.sin(2 * np.pi * freq * t)
        
        # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨
        envelope = np.exp(-t * 0.3)
        audio *= envelope
        
        return audio
    
    def get_audio_info(self, audio: np.ndarray) -> Dict[str, Any]:
        """éŸ³å£°æƒ…å ±ã‚’å–å¾—"""
        duration = len(audio) / self.sample_rate
        rms = np.sqrt(np.mean(audio**2))
        peak = np.max(np.abs(audio))
        
        # ãƒ‰ãƒŸãƒŠãƒ³ãƒˆå‘¨æ³¢æ•°ã‚’è¨ˆç®—
        fft = np.fft.fft(audio)
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        dominant_freq = freqs[np.argmax(np.abs(fft))]
        
        return {
            'duration': duration,
            'sample_rate': self.sample_rate,
            'rms': float(rms),
            'peak': float(peak),
            'dominant_frequency': float(abs(dominant_freq)),
            'length_samples': len(audio)
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
realistic_voice_generator = RealisticVoiceGenerator()

async def generate_voice(text: str) -> np.ndarray:
    """éŸ³å£°ã‚’ç”Ÿæˆï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return await realistic_voice_generator.generate_voice(text)

def get_audio_info(audio: np.ndarray) -> Dict[str, Any]:
    """éŸ³å£°æƒ…å ±ã‚’å–å¾—ï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return realistic_voice_generator.get_audio_info(audio)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆ
    async def test():
        text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"
        audio = await generate_voice(text)
        info = get_audio_info(audio)
        print(f"éŸ³å£°æƒ…å ±: {info}")
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        sf.write("test_realistic.wav", audio, 22050)
        print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: test_realistic.wav")
    
    asyncio.run(test())
